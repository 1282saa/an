#!/usr/bin/env python3
"""
ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Mock ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
articles = [
    {
        "title": "ì‚¼ì„±ì „ì 3ë¶„ê¸° ì‹¤ì  ë°œí‘œ",
        "content": "ì‚¼ì„±ì „ìê°€ 3ë¶„ê¸° ì‹¤ì ì„ ë°œí‘œí–ˆë‹¤. ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ì‚¬ì—…ì˜ ì„±ì¥ìœ¼ë¡œ ì˜ì—…ì´ìµì´ í¬ê²Œ ì¦ê°€í–ˆë‹¤.",
        "provider_name": "ì„œìš¸ê²½ì œ",
        "published_at": "2024-10-25",
        "provider_link_page": "http://example.com",
        "news_id": "1"
    },
    {
        "title": "ë°˜ë„ì²´ ì‹œì¥ ì „ë§ ë°ì•„",
        "content": "ì „ë¬¸ê°€ë“¤ì€ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ì‹œì¥ì˜ íšŒë³µì„¸ê°€ ì§€ì†ë  ê²ƒì´ë¼ê³  ì „ë§í–ˆë‹¤. ì‚¼ì„±ì „ìì™€ SKí•˜ì´ë‹‰ìŠ¤ì˜ ì‹¤ì  ê°œì„ ì´ ê¸°ëŒ€ëœë‹¤.",
        "provider_name": "í•œêµ­ê²½ì œ",
        "published_at": "2024-10-24", 
        "provider_link_page": "http://example.com",
        "news_id": "2"
    }
]

# í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
from collections import Counter
import re

def extract_keywords_from_articles(articles):
    keyword_pattern = re.compile(r'[ê°€-í£]{2,}')
    
    stopwords = {
        'ê¸°ì', 'ê¸°ì—…', 'íšŒì‚¬', 'ì‚¬ì—…', 'ì‹œì¥', 'ì •ë¶€', 'êµ­ê°€', 'ì§€ë‚œ', 'ì˜¬í•´', 'ë‚´ë…„',
        'ì´ë²ˆ', 'ë‹¹ì‹œ', 'í˜„ì¬', 'ê´€ë ¨', 'í†µí•´', 'ìœ„í•´', 'ëŒ€í•œ', 'êµ­ë‚´', 'í•´ì™¸', 'ì „ë…„',
        'ì´ë‚ ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì´í›„', 'ì´ì „', 'ë™ì•ˆ', 'ê³¼ì •', 'ê²°ê³¼', 'ìƒí™©'
    }
    
    all_keywords = []
    for article in articles:
        title = article.get("title", "")
        content = article.get("content", "")
        text = f"{title} {content}"
        
        keywords = keyword_pattern.findall(text)
        keywords = [kw for kw in keywords if kw not in stopwords and len(kw) <= 10]
        all_keywords.extend(keywords)
    
    keyword_counts = Counter(all_keywords).most_common(10)
    
    return [
        {
            "keyword": keyword,
            "count": count,
            "weight": count / max(keyword_counts[0][1], 1)
        }
        for keyword, count in keyword_counts
    ]

def generate_network_data(articles, keywords):
    nodes = []
    links = []
    
    # í‚¤ì›Œë“œ ë…¸ë“œ ìƒì„±
    for i, keyword_data in enumerate(keywords[:5]):
        nodes.append({
            "id": f"keyword_{i}",
            "name": keyword_data["keyword"],
            "type": "keyword", 
            "weight": keyword_data["weight"],
            "size": 10 + keyword_data["weight"] * 20,
            "color": "#3B82F6"
        })
    
    # ê¸°ì‚¬ ë…¸ë“œ ìƒì„±
    for i, article in enumerate(articles):
        nodes.append({
            "id": f"article_{i}",
            "name": article.get("title", "")[:30] + "...",
            "type": "article",
            "title": article.get("title", ""),
            "provider": article.get("provider_name", ""),
            "url": article.get("provider_link_page", ""),
            "published_at": article.get("published_at", ""),
            "size": 15,
            "color": "#EF4444"
        })
    
    # ê¸°ì‚¬-í‚¤ì›Œë“œ ê°„ ë§í¬ ìƒì„±
    for article_idx, article in enumerate(articles):
        title = article.get("title", "")
        content = article.get("content", "")
        text = f"{title} {content}".lower()
        
        for keyword_idx, keyword_data in enumerate(keywords[:5]):
            keyword = keyword_data["keyword"]
            if keyword in text:
                strength = text.count(keyword) * keyword_data["weight"]
                links.append({
                    "source": f"keyword_{keyword_idx}",
                    "target": f"article_{article_idx}",
                    "strength": strength,
                    "width": max(1, strength * 3)
                })
    
    return {
        "nodes": nodes,
        "links": links
    }

print("ğŸ” ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸")
print("=" * 50)

# í‚¤ì›Œë“œ ì¶”ì¶œ
keywords = extract_keywords_from_articles(articles)
print(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
for kw in keywords:
    print(f"  - {kw['keyword']}: {kw['count']}íšŒ (ê°€ì¤‘ì¹˜: {kw['weight']:.2f})")

# ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„±
network_data = generate_network_data(articles, keywords)
print(f"\nğŸ•¸ï¸ ë„¤íŠ¸ì›Œí¬ ë°ì´í„°:")
print(f"  - ë…¸ë“œ: {len(network_data['nodes'])}ê°œ")
print(f"  - ë§í¬: {len(network_data['links'])}ê°œ")

print(f"\nğŸ“Š ë…¸ë“œ ì •ë³´:")
for node in network_data['nodes']:
    print(f"  - {node['name']} ({node['type']}) - size: {node['size']}")

print(f"\nğŸ”— ë§í¬ ì •ë³´:")
for link in network_data['links']:
    print(f"  - {link['source']} â†’ {link['target']} (ê°•ë„: {link['strength']:.2f})")

print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.")