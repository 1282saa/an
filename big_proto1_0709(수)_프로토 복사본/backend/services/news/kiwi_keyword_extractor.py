"""
Kiwi ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œê¸°

ê¸°ì¡´ ë‹¨ìˆœ ë‹¨ì–´ ë¶„ë¦¬ ë°©ì‹ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ 
í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì •í™•í•œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

from kiwipiepy import Kiwi
from typing import List, Dict, Tuple, Set
import re
import time
from functools import lru_cache


class KiwiKeywordExtractor:
    """Kiwi ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™” ë° ì‚¬ìš©ì ì‚¬ì „ êµ¬ì¶•"""
        self.kiwi = Kiwi()
        self._setup_user_dictionary()
        
        # ê²€ìƒ‰ìš© í’ˆì‚¬ íƒœê·¸ ì •ì˜ (ì¤‘ìš”ë„ ìˆœ)
        self.search_tags = {
            'NNP': 1.0,  # ê³ ìœ ëª…ì‚¬ (ì‚¼ì„±ì „ì, ë„¤ì´ë²„) - ìµœê³  ì¤‘ìš”ë„
            'SL': 0.9,   # ì™¸êµ­ì–´ (HBM, AI, GPU)
            'NNG': 0.8,  # ì¼ë°˜ëª…ì‚¬ (ë°˜ë„ì²´, ìƒí™©)
            'SN': 0.6,   # ìˆ«ì (2024, 10%)
            'NR': 0.5    # ìˆ˜ì‚¬ (ì²«ì§¸, ë‘˜ì§¸)
        }
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ì—ì„œ ì œì™¸í•  ë¶ˆìš©ì–´
        self.stopwords = {
            # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´
            'ê²ƒ', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°',
            'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ê³³', 'ê·¸ê³³',
            'ë•Œë¬¸', 'ê²½ìš°', 'ìƒí™©', 'ë¬¸ì œ', 'ë°©ë²•', 'ê²°ê³¼', 'ê³¼ì •', 'ë‚´ìš©',
            'ì´ìœ ', 'ì›ì¸', 'ëª©ì ', 'íš¨ê³¼', 'ì˜í–¥', 'ë³€í™”', 'ì°¨ì´', 'ê´€ê³„',
            'ì •ë„', 'ìˆ˜ì¤€', 'ë²”ìœ„', 'ê·œëª¨', 'í¬ê¸°', 'ë†’ì´', 'ê¸¸ì´', 'ì‹œê°„',
            'ì´ë²ˆ', 'ì´ì „', 'ë‹¤ìŒ', 'ìµœê·¼', 'í˜„ì¬', 'ì•ìœ¼ë¡œ', 'ì´í›„', 'í–¥í›„',
            
            # ë‰´ìŠ¤ ê´€ë ¨ ë¶ˆìš©ì–´
            'ê¸°ì', 'ì·¨ì¬', 'ë³´ë„', 'ë°œí‘œ', 'ê³µê°œ', 'ë°œê°„', 'ê²Œì¬', 'ì†Œê°œ',
            'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì–¸ë¡ ', 'ë§¤ì²´', 'ì‹ ë¬¸', 'ë°©ì†¡', 'ì˜¨ë¼ì¸'
        }
        
        # ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸
        print("âœ… KiwiKeywordExtractor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_user_dictionary(self):
        """ë‰´ìŠ¤ ë„ë©”ì¸ íŠ¹í™” ì‚¬ìš©ì ì‚¬ì „ êµ¬ì¶•"""
        
        # ê¸°ìˆ /IT ìš©ì–´ - ê³ ìœ ëª…ì‚¬ë¡œ ì²˜ë¦¬
        tech_terms = [
            'HBM', 'GPU', 'CPU', 'AI', 'ChatGPT', 'LLM', 'API',
            'NFT', 'ë©”íƒ€ë²„ìŠ¤', 'VR', 'AR', 'IoT', '5G', '6G',
            'SaaS', 'PaaS', 'IaaS', 'AWS', 'Azure', 'GCP',
            'ML', 'DL', 'NLP', 'CV', 'AGI', 'ASI',
            'JavaScript', 'Python', 'Java', 'React', 'Vue',
            'GitHub', 'Docker', 'Kubernetes', 'DevOps'
        ]
        
        # ê²½ì œ/ê¸ˆìœµ ìš©ì–´ - ê³ ìœ ëª…ì‚¬ë¡œ ì²˜ë¦¬
        finance_terms = [
            'ESG', 'IPO', 'M&A', 'GDP', 'CPI', 'PPI',
            'KOSPI', 'KOSDAQ', 'NASDAQ', 'S&P500', 'DOW',
            'ETF', 'REITs', 'KRW', 'USD', 'EUR', 'JPY',
            'B2B', 'B2C', 'B2G', 'ROI', 'ROE', 'EBITDA',
            'VC', 'PE', 'IB', 'CB', 'BW', 'DR'
        ]
        
        # í•œêµ­ ì£¼ìš” ê¸°ì—…ëª… - ê³ ìœ ëª…ì‚¬ë¡œ ì²˜ë¦¬
        korean_companies = [
            'ì‚¼ì„±ì „ì', 'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'í˜„ëŒ€ì°¨', 'LGì „ì', 'SKí•˜ì´ë‹‰ìŠ¤',
            'í¬ìŠ¤ì½”', 'ì…€íŠ¸ë¦¬ì˜¨', 'ë°”ì´ì˜¤ë‹ˆì•„', 'NAVER', 'Kakao',
            'LGí™”í•™', 'SKì´ë…¸ë² ì´ì…˜', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'ê¸°ì•„',
            'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ì‚¼ì„±SDI', 'LGë””ìŠ¤í”Œë ˆì´',
            'SKí…”ë ˆì½¤', 'KT', 'LGìœ í”ŒëŸ¬ìŠ¤', 'ìš°ë¦¬ì€í–‰', 'KBê¸ˆìœµ',
            'ì‹ í•œê¸ˆìœµ', 'í•˜ë‚˜ê¸ˆìœµ', 'ë†í˜‘ê¸ˆìœµ', 'IBKê¸°ì—…ì€í–‰'
        ]
        
        # ê¸€ë¡œë²Œ ê¸°ì—…ëª… - ê³ ìœ ëª…ì‚¬ë¡œ ì²˜ë¦¬
        global_companies = [
            'Apple', 'Google', 'Microsoft', 'Amazon', 'Meta',
            'Tesla', 'Netflix', 'Spotify', 'Uber', 'Airbnb',
            'Twitter', 'Instagram', 'YouTube', 'TikTok',
            'NVIDIA', 'Intel', 'AMD', 'Qualcomm', 'TSMC',
            'Sony', 'Nintendo', 'Samsung', 'Huawei', 'Xiaomi'
        ]
        
        # ì‚°ì—…/ë¶„ì•¼ ìš©ì–´ - ì¼ë°˜ëª…ì‚¬ë¡œ ì²˜ë¦¬
        industry_terms = [
            'ë°˜ë„ì²´', 'ë°”ì´ì˜¤', 'í—¬ìŠ¤ì¼€ì–´', 'í•€í…Œí¬', 'ì—ë“€í…Œí¬',
            'í‘¸ë“œí…Œí¬', 'ì• ê·¸í…Œí¬', 'í´ë¦°í…Œí¬', 'ë¦¬í…Œì¼í…Œí¬',
            'ëª¨ë¹Œë¦¬í‹°', 'eì»¤ë¨¸ìŠ¤', 'ë””ì§€í„¸ì „í™˜', 'ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬',
            'ë¹…ë°ì´í„°', 'í´ë¼ìš°ë“œì»´í“¨íŒ…', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ë¡œë³´í‹±ìŠ¤'
        ]
        
        # ì‚¬ìš©ì ì‚¬ì „ì— ì¶”ê°€
        all_proper_nouns = tech_terms + finance_terms + korean_companies + global_companies
        for term in all_proper_nouns:
            self.kiwi.add_user_word(term, 'NNP')  # ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡
        
        for term in industry_terms:
            self.kiwi.add_user_word(term, 'NNG')  # ì¼ë°˜ëª…ì‚¬ë¡œ ë“±ë¡
        
        print(f"ğŸ“š ì‚¬ìš©ì ì‚¬ì „ ë“±ë¡ ì™„ë£Œ: {len(all_proper_nouns + industry_terms)}ê°œ ìš©ì–´")
    
    @lru_cache(maxsize=1000)
    def extract_keywords(self, query: str, min_length: int = 2, max_keywords: int = 10) -> List[str]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            query: ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
            min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´
            max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì¤‘ìš”ë„ ìˆœ)
        """
        if not query or not query.strip():
            return []
        
        # 1. ì „ì²˜ë¦¬
        query = self._preprocess_query(query)
        
        # 2. í˜•íƒœì†Œ ë¶„ì„
        try:
            result = self.kiwi.analyze(query)
            tokens = result[0][0]  # ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ì˜ í† í°ë“¤
        except Exception as e:
            print(f"âŒ í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self._fallback_extraction(query, min_length, max_keywords)
        
        # 3. í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ ë° ì ìˆ˜ ê³„ì‚°
        keyword_candidates = []
        for token in tokens:
            if (token.tag in self.search_tags and 
                len(token.form) >= min_length and
                token.form not in self.stopwords and
                self._is_valid_keyword(token.form)):
                
                # í’ˆì‚¬ë³„ ê°€ì¤‘ì¹˜ ì ìš©
                weight = self.search_tags[token.tag]
                # ê¸¸ì´ ë³´ë„ˆìŠ¤ (ê¸´ í‚¤ì›Œë“œì¼ìˆ˜ë¡ ì¤‘ìš”)
                weight += min(len(token.form) * 0.05, 0.3)
                
                keyword_candidates.append((token.form, weight))
        
        # 4. ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_keywords = {}
        for keyword, weight in keyword_candidates:
            if keyword in unique_keywords:
                unique_keywords[keyword] = max(unique_keywords[keyword], weight)
            else:
                unique_keywords[keyword] = weight
        
        # 5. ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(unique_keywords.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, _ in sorted_keywords[:max_keywords]]
    
    def extract_with_morphemes(self, query: str) -> List[Dict]:
        """
        í‚¤ì›Œë“œì™€ í•¨ê»˜ í˜•íƒœì†Œ ì •ë³´ë„ ë°˜í™˜
        
        Args:
            query: ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            í‚¤ì›Œë“œ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (í˜•íƒœì†Œ, í’ˆì‚¬, ì ìˆ˜ í¬í•¨)
        """
        if not query or not query.strip():
            return []
        
        query = self._preprocess_query(query)
        
        try:
            result = self.kiwi.analyze(query)
            tokens = result[0][0]
        except Exception as e:
            print(f"âŒ í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []
        
        keyword_info = []
        for token in tokens:
            if (token.tag in self.search_tags and 
                len(token.form) >= 2 and
                token.form not in self.stopwords and
                self._is_valid_keyword(token.form)):
                
                weight = self.search_tags[token.tag]
                weight += min(len(token.form) * 0.05, 0.3)
                
                keyword_info.append({
                    'keyword': token.form,
                    'pos': token.tag,
                    'weight': round(weight, 3),
                    'description': self._get_pos_description(token.tag)
                })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        keyword_info.sort(key=lambda x: x['weight'], reverse=True)
        return keyword_info
    
    def extract_for_news_search(self, query: str) -> Dict[str, List[str]]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            query: ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ëœ í‚¤ì›Œë“œ
        """
        keywords = self.extract_keywords(query, max_keywords=15)
        
        categorized = {
            'primary': [],      # ì£¼ìš” í‚¤ì›Œë“œ (ê³ ìœ ëª…ì‚¬, ì „ë¬¸ìš©ì–´)
            'secondary': [],    # ë³´ì¡° í‚¤ì›Œë“œ (ì¼ë°˜ëª…ì‚¬)
            'numeric': [],      # ìˆ«ì/ìˆ˜ì¹˜ ê´€ë ¨
            'all': keywords     # ì „ì²´ í‚¤ì›Œë“œ
        }
        
        for keyword in keywords:
            # í‚¤ì›Œë“œ ì¬ë¶„ì„í•˜ì—¬ í’ˆì‚¬ í™•ì¸
            try:
                result = self.kiwi.analyze(keyword)
                if result[0][0]:
                    main_pos = result[0][0][0].tag
                    
                    if main_pos == 'NNP' or main_pos == 'SL':
                        categorized['primary'].append(keyword)
                    elif main_pos == 'NNG':
                        categorized['secondary'].append(keyword)
                    elif main_pos in ['SN', 'NR']:
                        categorized['numeric'].append(keyword)
                    else:
                        categorized['secondary'].append(keyword)
            except:
                categorized['secondary'].append(keyword)
        
        return categorized
    
    def generate_search_queries(self, query: str) -> List[str]:
        """
        í‚¤ì›Œë“œë¥¼ ì¡°í•©í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        categorized = self.extract_for_news_search(query)
        search_queries = []
        
        # ì „ëµ 1: ì£¼ìš” í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        if categorized['primary']:
            if len(categorized['primary']) >= 2:
                # ì£¼ìš” í‚¤ì›Œë“œ 2ê°œ ì¡°í•©
                search_queries.append(f"{categorized['primary'][0]} AND {categorized['primary'][1]}")
            
            # ì£¼ìš” í‚¤ì›Œë“œ + ë³´ì¡° í‚¤ì›Œë“œ
            if categorized['secondary']:
                search_queries.append(f"{categorized['primary'][0]} AND {categorized['secondary'][0]}")
        
        # ì „ëµ 2: í¬ê´„ì  OR ê²€ìƒ‰
        if len(categorized['primary']) >= 2:
            primary_or = " OR ".join(categorized['primary'][:3])
            search_queries.append(f"({primary_or})")
        
        # ì „ëµ 3: ì „ì²´ í‚¤ì›Œë“œ AND ê²€ìƒ‰ (ì—„ê²©í•œ ê²€ìƒ‰)
        if len(categorized['all']) >= 2:
            search_queries.append(" AND ".join(categorized['all'][:3]))
        
        # ê¸°ë³¸ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        if not search_queries:
            search_queries.append(query)
        
        return search_queries
    
    def _preprocess_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì¼ë¶€ ìœ ì§€)
        query = re.sub(r'[^\w\sê°€-í£.%]', ' ', query)
        # ì—°ì† ê³µë°± ì œê±°
        query = re.sub(r'\s+', ' ', query).strip()
        return query
    
    def _is_valid_keyword(self, word: str) -> bool:
        """ìœ íš¨í•œ í‚¤ì›Œë“œì¸ì§€ ê²€ì¦"""
        # 1-3ìë¦¬ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸ (ì—°ë„ ë“± 4ìë¦¬ëŠ” í¬í•¨)
        if word.isdigit() and len(word) < 4:
            return False
        
        # í•œê¸€ ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if re.match(r'^[ã„±-ã…ã…-ã…£]+$', word):
            return False
        
        # ì˜ë¬¸ì 1ê¸€ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if re.match(r'^[a-zA-Z]$', word):
            return False
        
        # íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if re.match(r'^[^\wê°€-í£]+$', word):
            return False
        
        return True
    
    def _get_pos_description(self, pos_tag: str) -> str:
        """í’ˆì‚¬ íƒœê·¸ ì„¤ëª…"""
        descriptions = {
            'NNP': 'ê³ ìœ ëª…ì‚¬',
            'NNG': 'ì¼ë°˜ëª…ì‚¬', 
            'SL': 'ì™¸êµ­ì–´',
            'SN': 'ìˆ«ì',
            'NR': 'ìˆ˜ì‚¬'
        }
        return descriptions.get(pos_tag, 'ê¸°íƒ€')
    
    def _fallback_extraction(self, query: str, min_length: int, max_keywords: int) -> List[str]:
        """í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨ - ëŒ€ì²´ ë°©ë²• ì‚¬ìš©")
        
        # ê°„ë‹¨í•œ ë‹¨ì–´ ë¶„ë¦¬ ë° í•„í„°ë§
        words = re.findall(r'\b\w+\b', query)
        keywords = []
        
        for word in words:
            if (len(word) >= min_length and 
                word not in self.stopwords and
                self._is_valid_keyword(word)):
                keywords.append(word)
        
        return keywords[:max_keywords]
    
    def benchmark(self, test_queries: List[str]) -> Dict:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = {
            'total_queries': len(test_queries),
            'avg_time_ms': 0,
            'results': []
        }
        
        total_time = 0
        for query in test_queries:
            start_time = time.time()
            keywords = self.extract_keywords(query)
            end_time = time.time()
            
            processing_time = (end_time - start_time) * 1000
            total_time += processing_time
            
            results['results'].append({
                'query': query,
                'keywords': keywords,
                'time_ms': round(processing_time, 2)
            })
        
        results['avg_time_ms'] = round(total_time / len(test_queries), 2)
        return results


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    extractor = KiwiKeywordExtractor()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "ì‚¼ì„±ì „ìì™€ HBM ë°˜ë„ì²´ ìƒí™©",
        "ë„¤ì´ë²„ ChatGPT AI ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì¶œì‹œ",
        "í˜„ëŒ€ì°¨ì˜ ì „ê¸°ì°¨ 2024ë…„ íŒë§¤ ì‹¤ì ",
        "ì¹´ì¹´ì˜¤í†¡ì—ì„œ ë©”íƒ€ë²„ìŠ¤ ê¸°ëŠ¥ ì¶”ê°€",
        "SKí•˜ì´ë‹‰ìŠ¤ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ê¸€ë¡œë²Œ ì‹œì¥ ì§„ì¶œ"
    ]
    
    print("\nğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
    print("=" * 80)
    
    for query in test_queries:
        print(f"\nğŸ“ ì¿¼ë¦¬: {query}")
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extractor.extract_keywords(query)
        print(f"í‚¤ì›Œë“œ: {keywords}")
        
        # ìƒì„¸ ì •ë³´ í¬í•¨ ì¶”ì¶œ
        detailed = extractor.extract_with_morphemes(query)
        print("ìƒì„¸ ì •ë³´:")
        for info in detailed[:5]:  # ìƒìœ„ 5ê°œë§Œ
            print(f"  {info['keyword']} ({info['description']}) - {info['weight']}")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        search_queries = extractor.generate_search_queries(query)
        print(f"ê²€ìƒ‰ ì¿¼ë¦¬: {search_queries}")
        print("-" * 60) 