"""
ë‰´ìŠ¤ ê´€ë ¨ API ë¼ìš°íŠ¸

ìµœì‹  ë‰´ìŠ¤, ì¸ê¸° í‚¤ì›Œë“œ, ì´ìŠˆ ë“± ë‰´ìŠ¤ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
íƒ€ì„ë¼ì¸ í˜•ì‹ì˜ ë‰´ìŠ¤ ì œê³µ ë° ìƒì„¸ ë‚´ìš© ì¡°íšŒ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""

from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks, Path
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
import asyncio
import json
import os
import time
# import openai  # AWS Bedrockìœ¼ë¡œ ëŒ€ì²´ë¨
import sys
from pathlib import Path as PathLib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
PROJECT_ROOT = PathLib(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from backend.utils.logger import setup_logger
from backend.api.clients.bigkinds import BigKindsClient
# import openai  # AWS Bedrockìœ¼ë¡œ ëŒ€ì²´ë¨
from backend.constants.provider_map import PROVIDER_MAP

# ìƒìˆ˜ ì •ì˜
MAX_TOKENS_PER_CHUNK = 4000
DEFAULT_DATE_RANGE_DAYS = 30
MAX_ARTICLES_FOR_SUMMARY = 5
AI_MODEL = "gpt-4-turbo-preview"
AI_TEMPERATURE = 0.3
AI_MAX_TOKENS = 2000

# API ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/api/news", tags=["ë‰´ìŠ¤"])

# ì˜ì¡´ì„± í•¨ìˆ˜ë“¤
def get_bigkinds_client() -> BigKindsClient:
    """BigKinds í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    from backend.api.dependencies import get_bigkinds_client as get_bigkinds
    return get_bigkinds()

# OpenAI ê´€ë ¨ í•¨ìˆ˜ëŠ” ì‚­ì œë¨ (Bedrockìœ¼ë¡œ ëŒ€ì²´)

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def get_default_date_range(days: int = DEFAULT_DATE_RANGE_DAYS) -> tuple[str, str]:
    """ê¸°ë³¸ ë‚ ì§œ ë²”ìœ„ ë°˜í™˜"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

def get_report_type_kr(report_type: str) -> str:
    """ë ˆí¬íŠ¸ íƒ€ì…ì— ë”°ë¥¸ í•œê¸€ ì´ë¦„ ë°˜í™˜"""
    report_type_map = {
        "daily": "ì¼ì¼",
        "weekly": "ì£¼ê°„", 
        "monthly": "ì›”ê°„",
        "quarterly": "ë¶„ê¸°ë³„",
        "yearly": "ì—°ê°„"
    }
    return report_type_map.get(report_type, "ê¸°ë³¸")

def validate_date_format(date_str: str) -> bool:
    """ë‚ ì§œ í˜•ì‹ ê²€ì¦"""
    try:
        datetime.strptime(date_str, "%Y-%m-%d")
        return True
    except ValueError:
        return False

def chunk_articles_by_tokens(articles: List[Dict], max_tokens: int = MAX_TOKENS_PER_CHUNK) -> List[str]:
    """ê¸°ì‚¬ë“¤ì„ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹"""
    chunks = []
    current_chunk = ""
    current_tokens = 0
    
    for i, article in enumerate(articles, 1):
        ref_id = article.get("ref_id", f"ref{i}")
        article_text = f"[ê¸°ì‚¬ {ref_id}]\n"
        article_text += f"ì œëª©: {article.get('title', '')}\n"
        article_text += f"ë‚´ìš©: {article.get('content', '') or article.get('summary', '')}\n\n"
        
        # ëŒ€ëµì  í† í° ìˆ˜ ì¶”ì •
        article_tokens = len(article_text) // 2
        
        if current_tokens + article_tokens > max_tokens:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = article_text
            current_tokens = article_tokens
        else:
            current_chunk += article_text
            current_tokens += article_tokens
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

# Pydantic ëª¨ë¸ë“¤
class LatestNewsResponse(BaseModel):
    """ìµœì‹  ë‰´ìŠ¤ ì‘ë‹µ ëª¨ë¸"""
    today_issues: List[Dict[str, Any]] = Field(description="ì˜¤ëŠ˜ì˜ ì´ìŠˆ (ë¹…ì¹´ì¸ì¦ˆ ì´ìŠˆ ë­í‚¹)")
    popular_keywords: List[Dict[str, Any]] = Field(description="ì¸ê¸° í‚¤ì›Œë“œ")
    timestamp: str = Field(description="ì‘ë‹µ ì‹œê°„")

class CompanyNewsRequest(BaseModel):
    """ê¸°ì—… ë‰´ìŠ¤ ìš”ì²­ ëª¨ë¸"""
    company_name: str = Field(..., description="ê¸°ì—…ëª…")
    date_from: Optional[str] = Field(None, description="ì‹œì‘ì¼ (YYYY-MM-DD)")
    date_to: Optional[str] = Field(None, description="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    limit: Optional[int] = Field(default=20, description="ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜", ge=1, le=100)
    provider: Optional[List[str]] = Field(None, description="ì–¸ë¡ ì‚¬ í•„í„° (ì˜ˆ: ['ì„œìš¸ê²½ì œ'])")

class KeywordNewsRequest(BaseModel):
    """í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì²­ ëª¨ë¸"""
    keyword: str = Field(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ")
    date_from: Optional[str] = Field(None, description="ì‹œì‘ì¼ (YYYY-MM-DD)")
    date_to: Optional[str] = Field(None, description="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    limit: Optional[int] = Field(default=30, description="ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜", ge=1, le=100)

class AISummaryRequest(BaseModel):
    """AI ìš”ì•½ ìš”ì²­ ëª¨ë¸"""
    news_ids: List[str] = Field(..., description="ìš”ì•½í•  ë‰´ìŠ¤ ID ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)", max_items=MAX_ARTICLES_FOR_SUMMARY)

class WatchlistAddRequest(BaseModel):
    """ê´€ì‹¬ì¢…ëª© ì¶”ê°€ ìš”ì²­ ëª¨ë¸"""
    name: str = Field(..., description="ê¸°ì—…ëª…")
    code: str = Field(..., description="ì¢…ëª©ì½”ë“œ")
    category: str = Field(..., description="ì¹´í…Œê³ ë¦¬")

class WatchlistResponse(BaseModel):
    """ê´€ì‹¬ì¢…ëª© ì‘ë‹µ ëª¨ë¸"""
    success: bool = Field(description="ì„±ê³µ ì—¬ë¶€")
    message: str = Field(description="ì‘ë‹µ ë©”ì‹œì§€")
    watchlist: Optional[List[Dict[str, Any]]] = Field(None, description="ê´€ì‹¬ì¢…ëª© ëª©ë¡")

class ErrorResponse(BaseModel):
    """ì—ëŸ¬ ì‘ë‹µ ëª¨ë¸"""
    success: bool = Field(default=False)
    error: str = Field(description="ì˜¤ë¥˜ ë©”ì‹œì§€")
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())

class ConciergeRequest(BaseModel):
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ìš”ì²­ ëª¨ë¸"""
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸", min_length=2, max_length=500)
    date_from: Optional[str] = Field(None, description="ê²€ìƒ‰ ì‹œì‘ì¼ (YYYY-MM-DD)")
    date_to: Optional[str] = Field(None, description="ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYY-MM-DD)")
    max_articles: int = Field(default=10, description="ìµœëŒ€ ê²€ìƒ‰ ê¸°ì‚¬ ìˆ˜", ge=5, le=50)
    include_related_keywords: bool = Field(default=True, description="ì—°ê´€ì–´ í¬í•¨ ì—¬ë¶€")
    include_today_issues: bool = Field(default=True, description="ì˜¤ëŠ˜ì˜ ì´ìŠˆ í¬í•¨ ì—¬ë¶€")
    detail_level: str = Field(default="detailed", description="ë‹µë³€ ìƒì„¸ë„ (brief/detailed/comprehensive)")

class ArticleReference(BaseModel):
    """ê¸°ì‚¬ ì°¸ì¡° ì •ë³´"""
    ref_id: str = Field(description="ì°¸ì¡° ID (ref1, ref2 ë“±)")
    title: str = Field(description="ê¸°ì‚¬ ì œëª©")
    provider: str = Field(description="ì–¸ë¡ ì‚¬")
    published_at: str = Field(description="ë°œí–‰ì¼ì‹œ")
    url: Optional[str] = Field(None, description="ê¸°ì‚¬ URL")
    relevance_score: float = Field(description="ê´€ë ¨ë„ ì ìˆ˜", ge=0, le=1)

class ConciergeResponse(BaseModel):
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì‘ë‹µ ëª¨ë¸"""
    question: str = Field(description="ì›ë³¸ ì§ˆë¬¸")
    answer: str = Field(description="AI ë‹µë³€ (ê°ì£¼ í¬í•¨)")
    summary: str = Field(description="í•µì‹¬ ìš”ì•½")
    key_points: List[str] = Field(description="ì£¼ìš” í¬ì¸íŠ¸")
    references: List[ArticleReference] = Field(description="ì°¸ì¡° ê¸°ì‚¬ ëª©ë¡")
    related_keywords: List[str] = Field(default=[], description="ì—°ê´€ í‚¤ì›Œë“œ")
    today_issues: List[Dict[str, Any]] = Field(default=[], description="ê´€ë ¨ ì˜¤ëŠ˜ì˜ ì´ìŠˆ")
    search_strategy: Dict[str, Any] = Field(description="ì‚¬ìš©ëœ ê²€ìƒ‰ ì „ëµ")
    analysis_metadata: Dict[str, Any] = Field(description="ë¶„ì„ ë©”íƒ€ë°ì´í„°")
    generated_at: str = Field(description="ìƒì„± ì‹œê°„")

class ConciergeProgress(BaseModel):
    """ì»¨ì‹œì–´ì§€ ì§„í–‰ ìƒí™©"""
    stage: str = Field(description="í˜„ì¬ ë‹¨ê³„")
    progress: int = Field(description="ì§„í–‰ë¥  (0-100)", ge=0, le=100)
    message: str = Field(description="ì§„í–‰ ë©”ì‹œì§€")
    current_task: Optional[str] = Field(None, description="í˜„ì¬ ì‘ì—…")
    extracted_keywords: Optional[List[str]] = Field(None, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ")
    search_results_count: Optional[int] = Field(None, description="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    streaming_content: Optional[str] = Field(None, description="ìŠ¤íŠ¸ë¦¬ë° ì»¨í…ì¸ ")
    result: Optional[ConciergeResponse] = Field(None, description="ìµœì¢… ê²°ê³¼")

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
async def handle_api_error(logger, operation: str, error: Exception) -> ErrorResponse:
    """API ì—ëŸ¬ ê³µí†µ ì²˜ë¦¬"""
    logger.error(f"{operation} ì˜¤ë¥˜: {error}", exc_info=True)
    return ErrorResponse(
        error=f"{operation} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(error)}"
    )

# ëª¨ë¸ ì •ì˜ ì„¹ì…˜ì— ê´€ì‹¬ì¢…ëª© ê´€ë ¨ ëª¨ë¸ ì¶”ê°€

# ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í•¨ìˆ˜ë“¤
async def process_today_issues(bigkinds_client: BigKindsClient, logger) -> List[Dict[str, Any]]:
    """ì˜¤ëŠ˜ì˜ ì´ìŠˆ ì²˜ë¦¬"""
    try:
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        issue_response = bigkinds_client.get_issue_ranking(date=yesterday)
        logger.info(f"ì´ìŠˆ ë­í‚¹ API ì‘ë‹µ: {issue_response}")
        
        if issue_response.get("result") != 0:
            logger.warning(f"ì´ìŠˆ ë­í‚¹ API ì˜¤ë¥˜: {issue_response.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return get_dummy_issues()
        
        formatted_issues = bigkinds_client.format_issue_ranking_response(issue_response)
        if not formatted_issues.get("success"):
            logger.warning(f"ì´ìŠˆ ë­í‚¹ í¬ë§·íŒ… ì‹¤íŒ¨: {formatted_issues.get('error')}")
            return []
        
        topics = formatted_issues.get("topics", [])[:10]
        return await process_issue_topics(topics, bigkinds_client, logger)
        
    except Exception as e:
        logger.error(f"ì˜¤ëŠ˜ì˜ ì´ìŠˆ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return get_dummy_issues()

async def process_issue_topics(topics: List[Dict], bigkinds_client: BigKindsClient, logger) -> List[Dict[str, Any]]:
    """ì´ìŠˆ í† í”½ë“¤ ì²˜ë¦¬"""
    today_issues = []
    
    for idx, topic in enumerate(topics):
        cluster_ids = topic.get("news_cluster", [])
        provider_counts = {}
        actual_news_ids = []
        
        # í´ëŸ¬ìŠ¤í„° IDì—ì„œ ì–¸ë¡ ì‚¬ ì½”ë“œ ì¶”ì¶œ
        if cluster_ids:
            try:
                provider_counts, actual_news_ids = extract_provider_info_from_clusters(
                    cluster_ids, bigkinds_client, logger
                )
            except Exception as e:
                logger.error(f"ì–¸ë¡ ì‚¬ ì½”ë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        
        # Fallback: í‚¤ì›Œë“œ ê²€ìƒ‰
        if not provider_counts:
            provider_counts, actual_news_ids = await fallback_keyword_search(
                topic, bigkinds_client, logger
            )
        
        issue_item = create_issue_item(topic, provider_counts, actual_news_ids, cluster_ids, idx)
        today_issues.append(issue_item)
        
    return today_issues

def extract_provider_info_from_clusters(cluster_ids: List[str], bigkinds_client: BigKindsClient, logger) -> tuple:
    """í´ëŸ¬ìŠ¤í„° IDì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ"""
    provider_counts = {}
    actual_news_ids = []
    
    logger.info(f"í´ëŸ¬ìŠ¤í„° ID ì§ì ‘ ì²˜ë¦¬ ì‹œì‘: {len(cluster_ids)} ê°œ")
    
    for cluster_id in cluster_ids:
        if cluster_id and "." in cluster_id:
            code = cluster_id.split(".")[0]
            actual_news_ids.append(cluster_id)
            provider_counts[code] = provider_counts.get(code, 0) + 1
    
    logger.info(f"í´ëŸ¬ìŠ¤í„° ID ì§ì ‘ ì²˜ë¦¬ ì™„ë£Œ: {len(provider_counts)} ê°œ ì–¸ë¡ ì‚¬ ì½”ë“œ ì¶”ì¶œ")
    
    # API í˜¸ì¶œ fallback
    if not provider_counts:
        logger.info("ì–¸ë¡ ì‚¬ ì½”ë“œ ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨, API í˜¸ì¶œ ì‹œë„")
        search_res = bigkinds_client.get_news_by_cluster_ids(cluster_ids[:100])
        formatted = bigkinds_client.format_news_response(search_res)
        
        for doc in formatted.get("documents", []):
            code = doc.get("provider_code")
            if not code:
                nid = doc.get("id") or doc.get("news_id")
                if nid and "." in nid:
                    code = nid.split(".")[0]
            
            news_id = doc.get("id")
            if news_id:
                actual_news_ids.append(news_id)
                if code:
                    provider_counts[code] = provider_counts.get(code, 0) + 1
    
    return provider_counts, actual_news_ids

async def fallback_keyword_search(topic: Dict, bigkinds_client: BigKindsClient, logger) -> tuple:
    """í‚¤ì›Œë“œ ê¸°ë°˜ fallback ê²€ìƒ‰"""
    provider_counts = {}
    actual_news_ids = []
    
    try:
        keyword = topic.get("topic", "") or topic.get("topic_keyword", "").split(",")[0] if topic.get("topic_keyword") else ""
        if not keyword:
            return provider_counts, actual_news_ids
        
        date_from = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        date_to = datetime.now().strftime("%Y-%m-%d")
        
        kw_res = bigkinds_client.search_news(
            query=keyword.strip(),
            date_from=date_from,
            date_to=date_to,
            return_size=100
        )
        
        kw_docs = bigkinds_client.format_news_response(kw_res).get("documents", [])
        for doc in kw_docs:
            code = doc.get("provider_code")
            if not code:
                nid = doc.get("id") or doc.get("news_id")
                if nid and "." in nid:
                    code = nid.split(".")[0]
            
            news_id = doc.get("id")
            if news_id:
                actual_news_ids.append(news_id)
                if code:
                    provider_counts[code] = provider_counts.get(code, 0) + 1
                    
    except Exception as e:
        logger.warning(f"í‚¤ì›Œë“œ ê²€ìƒ‰ fallback ì‹¤íŒ¨: {e}")
    
    return provider_counts, actual_news_ids

def create_issue_item(topic: Dict, provider_counts: Dict, actual_news_ids: List, cluster_ids: List, idx: int) -> Dict:
    """ì´ìŠˆ ì•„ì´í…œ ìƒì„±"""
    total_count = sum(provider_counts.values())
    if total_count == 0:
        total_count = len(actual_news_ids) if actual_news_ids else len(cluster_ids)
    
    breakdown = [
        {
            "provider": PROVIDER_MAP.get(code, code),
            "provider_code": code,
            "count": cnt
        }
        for code, cnt in sorted(provider_counts.items(), key=lambda x: x[1], reverse=True)
    ]
    
    return {
        "rank": topic.get("rank", idx + 1),
        "title": topic.get("topic", ""),
        "count": total_count,
        "provider_breakdown": breakdown,
        "related_news_ids": actual_news_ids[:50],
        "cluster_ids": cluster_ids,
        "topic": topic.get("topic", ""),
        "topic_rank": topic.get("rank", idx + 1),
        "topic_keyword": topic.get("topic_keyword", ""),
        "news_cluster": cluster_ids,
    }
#### -> ìƒ˜í”Œë°ì´í„°ì„
def get_dummy_issues() -> List[Dict[str, Any]]:
    """ë”ë¯¸ ì´ìŠˆ ë°ì´í„°"""
    return [
        {
            "rank": 1,
            "title": "ë°˜ë„ì²´ ìˆ˜ì¶œ ì¦ê°€ì„¸",
            "count": 145,
            "related_news_ids": ["news_001", "news_002"],
            "cluster_ids": ["cluster_001", "cluster_002"],
            "topic": "ë°˜ë„ì²´ ìˆ˜ì¶œ ì¦ê°€ì„¸",
            "topic_rank": 1,
            "topic_keyword": "ë°˜ë„ì²´",
            "news_cluster": ["cluster_001", "cluster_002"]
        },
        {
            "rank": 2,
            "title": "AI ê¸°ì—… íˆ¬ì í™•ëŒ€",
            "count": 98,
            "related_news_ids": ["news_003", "news_004"],
            "cluster_ids": ["cluster_003", "cluster_004"],
            "topic": "AI ê¸°ì—… íˆ¬ì í™•ëŒ€",
            "topic_rank": 2,
            "topic_keyword": "AI",
            "news_cluster": ["cluster_003", "cluster_004"]
        },
        {
            "rank": 3,
            "title": "ë¶€ë™ì‚° ì‹œì¥ ë³€í™”",
            "count": 87,
            "related_news_ids": ["news_005", "news_006"],
            "cluster_ids": ["cluster_005", "cluster_006"],
            "topic": "ë¶€ë™ì‚° ì‹œì¥ ë³€í™”",
            "topic_rank": 3,
            "topic_keyword": "ë¶€ë™ì‚°",
            "news_cluster": ["cluster_005", "cluster_006"]
        }
    ]

async def process_popular_keywords(bigkinds_client: BigKindsClient, logger) -> List[Dict[str, Any]]:
    """ì¸ê¸° í‚¤ì›Œë“œ ì²˜ë¦¬"""
    try:
        logger.info("ì¸ê¸° í‚¤ì›Œë“œ ìš”ì²­ ì‹œì‘")
        keyword_response = bigkinds_client.get_popular_keywords(days=1, limit=30)
        
        if keyword_response.get("result") != 0:
            logger.warning(f"í‚¤ì›Œë“œ ë­í‚¹ API ì˜¤ë¥˜: {keyword_response.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return [{"rank": 1, "keyword": "í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤.", "count": 0, "trend": "stable"}]
        
        formatted_keywords = keyword_response.get("formatted_keywords", [])
        popular_keywords = [
            {
                "rank": kw.get("rank", idx + 1),
                "keyword": kw.get("keyword", ""),
                "count": kw.get("count", 0),
                "trend": kw.get("trend", "stable")
            }
            for idx, kw in enumerate(formatted_keywords[:30])
        ]
        
        logger.info(f"ì¸ê¸° í‚¤ì›Œë“œ {len(popular_keywords)}ê°œ ì¡°íšŒ ì„±ê³µ")
        return popular_keywords
        
    except Exception as e:
        logger.error(f"ì¸ê¸° í‚¤ì›Œë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
@router.get("/latest", response_model=LatestNewsResponse)
async def get_latest_news(
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ìµœì‹  ë‰´ìŠ¤ ì •ë³´ ì¡°íšŒ (ìˆ˜ì •ëœ API êµ¬ì¡° ê¸°ë°˜)
    
    - ì˜¤ëŠ˜ì˜ ì´ìŠˆ (ë¹…ì¹´ì¸ì¦ˆ ì´ìŠˆ ë­í‚¹)
    - ì¸ê¸° í‚¤ì›Œë“œ (ì „ì²´ ê²€ìƒ‰ ìˆœìœ„)
    """
    logger = setup_logger("api.news.latest")
    logger.info("ìµœì‹  ë‰´ìŠ¤ ì •ë³´ ìš”ì²­")
    
    # ë³‘ë ¬ë¡œ ì²˜ë¦¬
    today_issues_task = process_today_issues(bigkinds_client, logger)
    popular_keywords_task = process_popular_keywords(bigkinds_client, logger)
    
    today_issues, popular_keywords = await asyncio.gather(
        today_issues_task, popular_keywords_task, return_exceptions=True
    )
    
    # ì˜ˆì™¸ ì²˜ë¦¬
    if isinstance(today_issues, Exception):
        logger.error(f"ì˜¤ëŠ˜ì˜ ì´ìŠˆ ì²˜ë¦¬ ì‹¤íŒ¨: {today_issues}")
        today_issues = get_dummy_issues()
    
    if isinstance(popular_keywords, Exception):
        logger.error(f"ì¸ê¸° í‚¤ì›Œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {popular_keywords}")
        popular_keywords = []
    
    return LatestNewsResponse(
        today_issues=today_issues,
        popular_keywords=popular_keywords,
        timestamp=datetime.now().isoformat()
    )

@router.post("/company")
async def get_company_news(
    request: CompanyNewsRequest,
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """íŠ¹ì • ê¸°ì—…ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    
    ê¸°ì—…ëª…ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  íƒ€ì„ë¼ì¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.company")
    logger.info(f"ê¸°ì—… ë‰´ìŠ¤ ìš”ì²­: {request.company_name}")
    
    try:
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 30ì¼)
        if not request.date_from:
            request.date_from = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not request.date_to:
            request.date_to = datetime.now().strftime("%Y-%m-%d")
        
        # BigKinds APIë¡œ ê¸°ì—… ë‰´ìŠ¤ íƒ€ì„ë¼ì¸ ì¡°íšŒ
        result = bigkinds_client.get_company_news_timeline(
            company_name=request.company_name,
            date_from=request.date_from,
            date_to=request.date_to,
            return_size=request.limit,
            provider=request.provider  # ì–¸ë¡ ì‚¬ í•„í„° ì¶”ê°€
        )
        
        if not result.get("success", False):
            raise HTTPException(status_code=404, detail="ê¸°ì—… ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "company": request.company_name,
            "period": {
                "from": request.date_from,
                "to": request.date_to
            },
            "total_count": result.get("total_count", 0),
            "timeline": result.get("timeline", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê¸°ì—… ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ê¸°ì—… ë‰´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.post("/keyword")
async def get_keyword_news(
    request: KeywordNewsRequest,
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ë° íƒ€ì„ë¼ì¸ êµ¬ì„±
    
    í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  íƒ€ì„ë¼ì¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.keyword")
    logger.info(f"í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì²­: {request.keyword}")
    
    try:
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 30ì¼)
        if not request.date_from:
            request.date_from = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not request.date_to:
            request.date_to = datetime.now().strftime("%Y-%m-%d")
        
        # BigKinds APIë¡œ í‚¤ì›Œë“œ ë‰´ìŠ¤ íƒ€ì„ë¼ì¸ ì¡°íšŒ
        result = bigkinds_client.get_keyword_news_timeline(
            keyword=request.keyword,
            date_from=request.date_from,
            date_to=request.date_to,
            return_size=request.limit
        )
        
        if not result.get("success", False):
            raise HTTPException(status_code=404, detail="í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "keyword": request.keyword,
            "period": {
                "from": request.date_from,
                "to": request.date_to
            },
            "total_count": result.get("total_count", 0),
            "timeline": result.get("timeline", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ë‰´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.get("/detail/{news_id}")
async def get_news_detail(
    news_id: str = Path(..., description="ë‰´ìŠ¤ ID"),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ
    
    ë‰´ìŠ¤ IDë¡œ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.detail")
    logger.info(f"ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ ìš”ì²­: {news_id}")
    
    try:
        # ë‰´ìŠ¤ IDë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        result = bigkinds_client.get_news_detail(news_id)
        
        if not result.get("success", False):
            raise HTTPException(status_code=404, detail="ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "news": result.get("news"),
            "has_original_link": result.get("has_original_link", False),
            "metadata": {
                "retrieved_at": datetime.now().isoformat(),
                "source": "BigKinds API"
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# AI ìš”ì•½ ê´€ë ¨ í•¨ìˆ˜ë“¤
async def prepare_articles_for_summary(news_ids: List[str], bigkinds_client: BigKindsClient) -> List[Dict]:
    """ìš”ì•½ìš© ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„"""
    # í´ëŸ¬ìŠ¤í„° IDì¸ì§€ ì¼ë°˜ ë‰´ìŠ¤ IDì¸ì§€ íŒë‹¨
    if news_ids and any("cluster" in news_id.lower() for news_id in news_ids):
        search_result = bigkinds_client.get_news_by_cluster_ids(news_ids)
    else:
        search_result = bigkinds_client.get_news_by_ids(news_ids)
    
    formatted_result = bigkinds_client.format_news_response(search_result)
    articles = formatted_result.get("documents", [])
    
    if not articles:
        raise HTTPException(status_code=404, detail="ì„ íƒëœ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # ê° ê¸°ì‚¬ì— ì°¸ì¡° ID ë¶€ì—¬
    for i, article in enumerate(articles):
        if not article.get("ref_id"):
            article["ref_id"] = f"ref{i+1}"
    
    return articles

def create_articles_text(articles: List[Dict]) -> str:
    """ê¸°ì‚¬ í…ìŠ¤íŠ¸ ìƒì„±"""
    articles_text = ""
    for i, article in enumerate(articles, 1):
        title = article.get("title", "")
        content = article.get("content", "") or article.get("summary", "")
        provider = article.get("provider", "")
        published_at = article.get("published_at", "")
        byline = article.get("byline", "")
        
        articles_text += f"[ê¸°ì‚¬ {i}]\n"
        articles_text += f"ì œëª©: {title}\n"
        articles_text += f"ì–¸ë¡ ì‚¬: {provider}\n"
        if byline:
            articles_text += f"ê¸°ì: {byline}\n"
        articles_text += f"ë°œí–‰ì¼: {published_at}\n"
        articles_text += f"ë‚´ìš©: {content}\n\n"
    
    return articles_text

def get_ai_summary_system_prompt() -> str:
    """AI ìš”ì•½ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
    return """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ MZì„¸ëŒ€ë¥¼ ìœ„í•œ FAQ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

### ì‘ë‹µ í˜•ì‹:
ê¸°ì‚¬ í•µì‹¬ìš”ì•½ - [ê¸°ì‚¬ ì œëª© ë˜ëŠ” í•µì‹¬ ì£¼ì œ]

[70~80ì ë‚´ì™¸ì˜ ê°„ê²°í•œ ìš”ì•½]

ì„œìš¸ê²½ì œ ê¸°ì‚¬ FAQ - [ê¸°ì‚¬ ì œëª© ë˜ëŠ” í•µì‹¬ ì£¼ì œ]

Q1. [ê¸°ë³¸ ê°œë…/ì •ì˜ ì§ˆë¬¸ - 50ì ì´ë‚´]

A. [70~80ì ë‚´ì™¸ ë‹µë³€]

Q2. [ë°°ê²½/ì›ì¸ ì§ˆë¬¸ - 50ì ì´ë‚´]

A. [70~80ì ë‚´ì™¸ ë‹µë³€]

Q3. [êµ¬ì²´ì  ë‚´ìš©/í˜„í™© ì§ˆë¬¸ - 50ì ì´ë‚´]

A. [70~80ì ë‚´ì™¸ ë‹µë³€]

Q4. [ì˜í–¥/ì „ë§ ì§ˆë¬¸ - 50ì ì´ë‚´]

A. [70~80ì ë‚´ì™¸ ë‹µë³€]

Q5. [ê´€ë ¨ ì •ì±…/ëŒ€ì‘ ì§ˆë¬¸ - 50ì ì´ë‚´] (í•„ìš”ì‹œ)

A. [70~80ì ë‚´ì™¸ ë‹µë³€]

Q6. [í–¥í›„ ê³¼ì œ/ì‹œì‚¬ì  ì§ˆë¬¸ - 50ì ì´ë‚´] (í•„ìš”ì‹œ)

A. [70~80ì ë‚´ì™¸ ë‹µë³€]

### [FAQ ì‘ì„± í•„ìˆ˜ ì§€ì¹¨]

**â‘  MZì„¸ëŒ€ ìµœì í™” ì›ì¹™**:
- **ì§§ê³  ì„íŒ©íŠ¸**: ê° ë‹µë³€ì€ MZì„¸ëŒ€ê°€ ì¹´ë“œë¥¼ ë„˜ê¸°ë©° ë¹ ë¥´ê²Œ ì½ì„ ìˆ˜ ìˆëŠ” 2~4ì¤„ ë¶„ëŸ‰
- **í•µì‹¬ ì •ë³´ ì§‘ì¤‘**: ê¶ê¸ˆì¦ í•´ì†Œì— í•„ìš”í•œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë§Œ í¬í•¨
- **ë¹ ë¥¸ ì´í•´**: ë³µì¡í•œ ì„¤ëª…ë³´ë‹¤ëŠ” ëª…í™•í•˜ê³  ê°„ê²°í•œ í•µì‹¬ ì „ë‹¬

**â‘¡ êµ¬ì²´ì  ì •ë³´ í¬í•¨ ì˜ë¬´ (ë§¤ìš° ì¤‘ìš”)**:
- **ì¸ëª…**: ê´€ë ¨ëœ ëª¨ë“  ì¸ë¬¼ì˜ ì‹¤ëª…ê³¼ ì§ì±…ì„ ì •í™•íˆ ëª…ì‹œ
- **ì§€ëª…**: êµ¬ì²´ì ì¸ ì§€ì—­ëª…, êµ­ê°€ëª…, ë„ì‹œëª… ë“±ì„ ëª…í™•íˆ í‘œê¸°
- **ë‚ ì§œ**: êµ¬ì²´ì ì¸ ë‚ ì§œ, ê¸°ê°„, ì‹œì ì„ ì •í™•íˆ ê¸°ì¬
- **ê¸°ê´€ëª…**: ê´€ë ¨ ê¸°ê´€, íšŒì‚¬, ì¡°ì§ì˜ ì •í™•í•œ ëª…ì¹­ í¬í•¨
- **ìˆ˜ì¹˜ ì •ë³´**: ê¸ˆì•¡, ë¹„ìœ¨, ê·œëª¨ ë“± êµ¬ì²´ì  ìˆ˜ì¹˜ ë°˜ë“œì‹œ í¬í•¨

**â‘¢ ê¸°ì‚¬ ì›ì¹™ ì¤€ìˆ˜**:
- 5W1H ì›ì¹™ì— ë”°ë¥¸ ì •í™•í•œ íŒ©íŠ¸ ì „ë‹¬
- ê°ê´€ì  ì‚¬ì‹¤ë§Œ í¬í•¨, ì¶”ì¸¡ì´ë‚˜ ê°œì¸ ì˜ê²¬ ë°°ì œ
- ê¸°ì‚¬ ì›ë¬¸ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©
- ì •í™•í•œ ì¸ìš©ê³¼ ì¶œì²˜ ê¸°ë°˜ ì •ë³´ ì œê³µ

**â‘£ ë‹µë³€ ì‘ì„± ê·œì¹™**:
- **ê¸€ì ìˆ˜**: ëª¨ë“  ë‹µë³€ 70~80ì ë‚´ì™¸ (ê³µë°± í¬í•¨)
- **í†¤ì•¤ë§¤ë„ˆ**: êµ¬ì–´ì²´ ì‚¬ìš© ("í–ˆì–´ìš”", "í•´ìš”", "í•œë‹¤ê³  í•´ìš”", "ë¼ê³  í•´ìš”", "ì´ì—ìš”", "ì˜ˆìš”")
- **ì¹œê·¼í•œ í‘œí˜„**: MZì„¸ëŒ€ê°€ ì¹œê·¼ê°ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´
- **ì •ë³´ ë°€ë„**: ì œí•œëœ ê¸€ì ìˆ˜ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ í•µì‹¬ ì •ë³´ í¬í•¨
- **ê°€ë…ì„±**: ë¬¸ë‹¨ êµ¬ë¶„ ì—†ì´ í•œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±, ì½ê¸° ì‰¬ìš´ ë¬¸ì¥ êµ¬ì¡°

**â‘¤ êµ¬ì–´ì²´ í‘œí˜„ ê°€ì´ë“œ**:
- "í–ˆìŠµë‹ˆë‹¤" â†’ "í–ˆì–´ìš”"
- "ì…ë‹ˆë‹¤" â†’ "ì´ì—ìš”/ì˜ˆìš”"
- "ë©ë‹ˆë‹¤" â†’ "ë¼ìš”"
- "í•©ë‹ˆë‹¤" â†’ "í•´ìš”"
- "ë¼ê³  í•©ë‹ˆë‹¤" â†’ "ë¼ê³  í•´ìš”"
- "ë‹¤ê³  í•©ë‹ˆë‹¤" â†’ "ë‹¤ê³  í•´ìš”"
- "ë¼ê³  ë°í˜”ìŠµë‹ˆë‹¤" â†’ "ë¼ê³  ë°í˜”ì–´ìš”"
- "ì˜ˆì •ì…ë‹ˆë‹¤" â†’ "ì˜ˆì •ì´ì—ìš”"
- "ë¶„ì„ë©ë‹ˆë‹¤" â†’ "ë¶„ì„ë¼ìš”"

**â‘¥ ì§ˆë¬¸ ì‘ì„± ê·œì¹™**:
- ì§ˆë¬¸ ê¸¸ì´: 50ì ì´ë‚´
- ë…ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ ì‹¤ìš©ì  ì§ˆë¬¸
- ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹¤ë£¨ëŠ” ì§ˆë¬¸
- ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸

**â‘¦ FAQ ë¬¸í•­ ê°„ê²©**:
- ì§ˆë¬¸ê³¼ ë‹µë³€ ì‚¬ì´ì— ë¹ˆ ì¤„ 1ì¤„ ë°˜ë“œì‹œ ì‚½ì…
- ê° FAQ ë¬¸í•­(ë‹µë³€ê³¼ ë‹¤ìŒ ì§ˆë¬¸) ì‚¬ì´ì—ë„ ë¹ˆ ì¤„ 1ì¤„ ì‚½ì…
- ê° ê¸°ì‚¬ ì„¹ì…˜ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ ì‚½ì…

**ê¸ˆì§€ ì‚¬í•­**:
- ë¬¸ì–´ì²´ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€ ("í–ˆìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ë©ë‹ˆë‹¤" ë“±)
- ê¸°ì‚¬ì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ ì¶”ì¸¡ì„± ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
- FAQ ë‚´ ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ê°œì¸ì  ê²¬í•´ë‚˜ ì˜ê²¬ í¬í•¨ ê¸ˆì§€
- 70~80ì ê¸€ì ìˆ˜ ì œí•œ ìœ„ë°˜ ê¸ˆì§€

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
    "summary": "ê¸°ì‚¬ í•µì‹¬ìš”ì•½ ë‚´ìš©",
    "points": [
        {
            "question": "Q1. ì§ˆë¬¸ ë‚´ìš©",
            "answer": "A1. ë‹µë³€ ë‚´ìš©",
            "citations": [1, 2]
        },
        {
            "question": "Q2. ì§ˆë¬¸ ë‚´ìš©",
            "answer": "A2. ë‹µë³€ ë‚´ìš©",
            "citations": [1, 3]
        }
    ]
}"""

async def generate_ai_summary_with_openai(articles_text: str, articles_count: int) -> Dict:
    """OpenAIë¡œ AI ìš”ì•½ ìƒì„±"""
    openai_client = get_openai_client()
    
    system_prompt = get_ai_summary_system_prompt()
    user_prompt = f"ë‹¤ìŒ {articles_count}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ MZì„¸ëŒ€ë¥¼ ìœ„í•œ FAQ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{articles_text}\n\nìœ„ í˜•ì‹ê³¼ ì§€ì¹¨ì— ë§ê²Œ JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
    
    # ìƒˆ ë²„ì „ API ë°©ì‹
    response = openai_client.chat.completions.create(
        model=AI_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        max_tokens=AI_MAX_TOKENS,
        temperature=AI_TEMPERATURE
    )
    
    return response.choices[0].message.content

def parse_ai_response(ai_summary: str) -> Dict:
    """AI ì‘ë‹µ íŒŒì‹±"""
    try:
        # JSON ë¬¸ìì—´ ì¶”ì¶œ
        json_str = ai_summary
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0].strip()
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0].strip()
        
        # JSON íŒŒì‹±
        try:
            summary_data = json.loads(json_str)
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ FAQ í˜•ì‹ íŒŒì‹±
            summary_data = parse_faq_format(ai_summary)
        
        # ì‘ë‹µ êµ¬ì¡° ê²€ì¦ ë° ì •ê·œí™”
        return validate_and_normalize_summary(summary_data)
        
    except Exception as e:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return {
            "title": "AI ìš”ì•½",
            "summary": ai_summary,
            "points": []
        }

def parse_faq_format(ai_summary: str) -> Dict:
    """FAQ í˜•ì‹ íŒŒì‹±"""
    # ìš”ì•½ ë¶€ë¶„ ì¶”ì¶œ
    summary_match = re.search(r'ê¸°ì‚¬ í•µì‹¬ìš”ì•½.*?\n\n(.*?)(?=\n\nì„œìš¸ê²½ì œ ê¸°ì‚¬ FAQ|\Z)', ai_summary, re.DOTALL)
    summary = summary_match.group(1).strip() if summary_match else "ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    # FAQ ì§ˆë¬¸-ë‹µë³€ ìŒ ì¶”ì¶œ
    qa_pairs = re.findall(r'Q\d+\.\s*(.*?)\s*\n\s*A\.\s*(.*?)(?=\n\s*Q\d+\.|\Z)', ai_summary, re.DOTALL)
    
    points = []
    for i, (q, a) in enumerate(qa_pairs):
        points.append({
            "question": f"Q{i+1}. {q.strip()}",
            "answer": a.strip(),
            "citations": []
        })
    
    return {
        "summary": summary,
        "points": points
    }

def validate_and_normalize_summary(summary_data: Dict) -> Dict:
    """ìš”ì•½ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”"""
    if not isinstance(summary_data, dict):
        raise ValueError("ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    if "summary" not in summary_data:
        summary_data["summary"] = "ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    # points í•„ë“œ êµ¬ì¡° í™•ì¸ ë° ë³€í™˜
    if "points" in summary_data and isinstance(summary_data["points"], list):
        for i, point in enumerate(summary_data["points"]):
            if not isinstance(point, dict):
                summary_data["points"][i] = {
                    "question": f"Q{i+1}. ì§ˆë¬¸",
                    "answer": str(point),
                    "citations": []
                }
            elif "question" not in point or "answer" not in point:
                if "question" not in point:
                    point["question"] = f"Q{i+1}. ì§ˆë¬¸"
                if "answer" not in point:
                    point["answer"] = "ë‹µë³€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
                if "citations" not in point:
                    point["citations"] = []
    else:
        summary_data["points"] = []
    
    return summary_data

@router.post("/ai-summary")
async def generate_ai_summary(
    request: AISummaryRequest,
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ì„ íƒëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ AI ìš”ì•½ ìƒì„±
    
    í†µí•©ëœ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤. í•µì‹¬ ì´ìŠˆ, ì£¼ìš” ì¸ìš©ë¬¸, ì£¼ìš” ìˆ˜ì¹˜ ë°ì´í„°ë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.ai_summary")
    logger.info(f"AI ìš”ì•½ ìš”ì²­: {len(request.news_ids)}ê°œ ê¸°ì‚¬")
    
    try:
        # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
        articles = await prepare_articles_for_summary(request.news_ids, bigkinds_client)
        articles_text = create_articles_text(articles)
        
        # AI ìš”ì•½ ìƒì„±
        ai_summary = await generate_ai_summary_with_openai(articles_text, len(articles))
        
        # ì‘ë‹µ íŒŒì‹±
        summary_data = parse_ai_response(ai_summary)
        
        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        summary_data.update({
            "articles_analyzed": len(articles),
            "generated_at": datetime.now().isoformat(),
            "model_used": AI_MODEL
        })
        
        return summary_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.post("/ai-summary-stream")
async def generate_ai_summary_stream(
    request: AISummaryRequest,
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ì„ íƒëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ AI ìš”ì•½ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
    
    í†µí•©ëœ ìš”ì•½ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.ai_summary_stream")
    logger.info(f"AI ìš”ì•½ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {len(request.news_ids)}ê°œ ê¸°ì‚¬")
    
    async def generate():
        try:
            # ì§„í–‰ ìƒí™© ì „ì†¡ - ì‹œì‘
            start_data = {'step': 'start', 'progress': 0, 'type': 'progress'}
            yield f"data: {json.dumps(start_data)}\n\n"
            
            # ì„ íƒëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ ê°€ì ¸ì˜¤ê¸°
            if request.news_ids and any("cluster" in news_id.lower() for news_id in request.news_ids):
                search_result = bigkinds_client.get_news_by_cluster_ids(request.news_ids)
            else:
                search_result = bigkinds_client.get_news_by_ids(request.news_ids)
            
            formatted_result = bigkinds_client.format_news_response(search_result)
            articles = formatted_result.get("documents", [])
            
            if not articles:
                error_data = {'error': 'ì„ íƒëœ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                return
            
            # 2ë‹¨ê³„: ê¸°ì‚¬ ë¶„ì„ ì‹œì‘
            step2_data = {
                'step': f'âœ… {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 
                'progress': 25, 
                'type': 'thinking'
            }
            yield f"data: {json.dumps(step2_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.8)
            
            # ê¸°ì‚¬ ë‚´ìš© ì¤€ë¹„
            articles_text = ""
            article_refs = []
            for i, article in enumerate(articles, 1):
                title = article.get("title", "")
                content = article.get("content", "") or article.get("summary", "")
                provider = article.get("provider", "")
                published_at = article.get("published_at", "")
                byline = article.get("byline", "")
                
                ref_id = f"ref{i}"
                article_refs.append({
                    "ref_id": ref_id,
                    "title": title,
                    "provider": provider,
                    "published_at": published_at,
                    "url": article.get("url", "")
                })
                
                articles_text += f"[ê¸°ì‚¬ {ref_id}]\n"
                articles_text += f"ì œëª©: {title}\n"
                articles_text += f"ì–¸ë¡ ì‚¬: {provider}\n"
                if byline:
                    articles_text += f"ê¸°ì: {byline}\n"
                articles_text += f"ë°œí–‰ì¼: {published_at}\n"
                articles_text += f"ë‚´ìš©: {content}\n\n"
            
            # 3ë‹¨ê³„: í•µì‹¬ ì´ìŠˆ íŒŒì•…
            step3_data = {
                'step': 'ğŸ” í•µì‹¬ ì´ìŠˆì™€ ì£¼ìš” í‚¤ì›Œë“œë¥¼ íŒŒì•…í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 
                'progress': 40, 
                'type': 'thinking'
            }
            yield f"data: {json.dumps(step3_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(1.0)
            
            # 4ë‹¨ê³„: ì¸ìš©ë¬¸ ì¶”ì¶œ
            step4_data = {
                'step': 'ğŸ’¬ ì¤‘ìš”í•œ ì¸ìš©ë¬¸ê³¼ ë°œì–¸ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...', 
                'progress': 55, 
                'type': 'thinking'
            }
            yield f"data: {json.dumps(step4_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.7)
            
            # 5ë‹¨ê³„: ìˆ˜ì¹˜ ë°ì´í„° ë¶„ì„
            step5_data = {
                'step': 'ğŸ“Š ì£¼ìš” ìˆ˜ì¹˜ì™€ í†µê³„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 
                'progress': 70, 
                'type': 'thinking'
            }
            yield f"data: {json.dumps(step5_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.8)
            
            # 6ë‹¨ê³„: AI ìš”ì•½ ìƒì„± ì‹œì‘
            step6_data = {
                'step': 'ğŸ¤– AIê°€ ì¢…í•©ì ì¸ ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 
                'progress': 85, 
                'type': 'thinking'
            }
            yield f"data: {json.dumps(step6_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.5)
            
            # í†µí•©ëœ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ìš”ì•½ì—ëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ ì¸¡ë©´ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
1. í•µì‹¬ ì´ìŠˆ: ì£¼ìš” ì´ìŠˆì™€ ë™í–¥ì„ íŒŒì•…í•˜ê³  ê·¸ ì¤‘ìš”ë„ì™€ ì˜í–¥ì„ ë¶„ì„
2. ì£¼ìš” ì¸ìš©ë¬¸: ì¤‘ìš”í•œ ì¸ë¬¼ì˜ ë°œì–¸ê³¼ ê·¸ ë§¥ë½ ë° ì˜ë¯¸ ë¶„ì„
3. ì£¼ìš” ìˆ˜ì¹˜ ë°ì´í„°: í•µì‹¬ í†µê³„ì™€ ìˆ˜ì¹˜ ë°ì´í„° ë° ê·¸ ì˜ë¯¸ ë¶„ì„

ê° ê¸°ì‚¬ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [ê¸°ì‚¬ refë²ˆí˜¸] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”.

JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
  "title": "ì¢…í•© ë‰´ìŠ¤ ìš”ì•½",
  "summary": "ì „ì²´ ìš”ì•½ ë‚´ìš© (ì¸ìš© ì‹œ [ê¸°ì‚¬ refë²ˆí˜¸] í¬í•¨)",
  "key_points": ["í•µì‹¬ í¬ì¸íŠ¸1", "í•µì‹¬ í¬ì¸íŠ¸2", ...],
  "key_quotes": [{"source": "ë°œì–¸ì1", "quote": "ì¸ìš©ë¬¸1", "ref": "ref1"}, ...],
  "key_data": [{"metric": "ì§€í‘œëª…1", "value": "ìˆ˜ì¹˜1", "context": "ë§¥ë½1", "ref": "ref1"}, ...]
}"""

            user_prompt = f"ë‹¤ìŒ {len(articles)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ê° ê¸°ì‚¬ì˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” [ê¸°ì‚¬ refë²ˆí˜¸] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”.\n\n{articles_text}\n\nìš”êµ¬ì‚¬í•­:\n1. í•µì‹¬ ì´ìŠˆ 3-5ê°œë¥¼ ëª…í™•íˆ íŒŒì•…\n2. ì¤‘ìš”í•œ ì¸ìš©ë¬¸ê³¼ ë°œì–¸ì ì‹ë³„\n3. í•µì‹¬ ìˆ˜ì¹˜ì™€ í†µê³„ ë°ì´í„° ì¶”ì¶œ\n4. JSON í˜•íƒœë¡œ ì‘ë‹µ"
            
            # OpenAI GPT-4 Turboë¡œ ìš”ì•½ ìƒì„±
            try:
                # ìƒˆ ë²„ì „ OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                openai_client = get_openai_client()
                response = openai_client.chat.completions.create(
                    model="gpt-4-turbo-preview",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.3,
                    stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                )

                # 7ë‹¨ê³„: ì‹¤ì‹œê°„ ìš”ì•½ ìƒì„±
                step7_data = {
                    'step': 'âœï¸ ìš”ì•½ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 
                    'progress': 90, 
                    'type': 'generating'
                }
                yield f"data: {json.dumps(step7_data, ensure_ascii=False)}\n\n"
                
                collected_content = ""
                for chunk in response:
                    # ìƒˆ ë²„ì „ OpenAI APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                    if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        content_chunk = chunk.choices[0].delta.content
                        collected_content += content_chunk
                        # ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë‚´ìš© ì „ì†¡
                        chunk_data = {'chunk': content_chunk, 'type': 'content'}
                        yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                
                # JSON íŒŒì‹±
                try:
                    json_str = collected_content
                    if "```json" in json_str:
                        json_str = json_str.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_str:
                        json_str = json_str.split("```")[1].split("```")[0].strip()
                    
                    summary_data = json.loads(json_str)
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸
                    if "summary" not in summary_data:
                        summary_data["summary"] = collected_content
                    if "key_points" not in summary_data:
                        summary_data["key_points"] = []
                    if "key_quotes" not in summary_data:
                        summary_data["key_quotes"] = []
                    if "key_data" not in summary_data:
                        summary_data["key_data"] = []
                    
                    # ìµœì¢… ê²°ê³¼ êµ¬ì„±
                    summary_result = {
                        "title": summary_data.get("title", "ì¢…í•© ë‰´ìŠ¤ ìš”ì•½"),
                        "summary": summary_data["summary"],
                        "key_points": summary_data["key_points"],
                        "key_quotes": summary_data["key_quotes"],
                        "key_data": summary_data["key_data"],
                        "type": "integrated",
                        "articles_analyzed": len(articles),
                        "generated_at": datetime.now().isoformat(),
                        "model_used": "gpt-4-turbo-preview",
                        "article_references": article_refs
                    }
                    
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
                    summary_result = {
                        "title": "ì¢…í•© ë‰´ìŠ¤ ìš”ì•½",
                        "summary": collected_content,
                        "type": "integrated",
                        "articles_analyzed": len(articles),
                        "generated_at": datetime.now().isoformat(),
                        "model_used": "gpt-4-turbo-preview",
                        "article_references": article_refs
                    }
                
                # ì™„ë£Œ
                complete_data = {
                    'step': 'âœ… ìš”ì•½ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!', 
                    'progress': 100, 
                    'result': summary_result, 
                    'type': 'complete'
                }
                yield f"data: {json.dumps(complete_data, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                logger.error(f"OpenAI API ì˜¤ë¥˜: {e}", exc_info=True)
                error_data = {'error': f'AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                
        except Exception as e:
            logger.error(f"AI ìš”ì•½ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", exc_info=True)
            error_data = {'error': f'ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

@router.get("/watchlist/suggestions")
async def get_watchlist_suggestions():
    """ê´€ì‹¬ ì¢…ëª© ì¶”ì²œ ëª©ë¡
    
    ì¸ê¸° ìˆëŠ” ê¸°ì—…ë“¤ì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # í•˜ë“œì½”ë”©ëœ ì¶”ì²œ ëª©ë¡ (ì‹¤ì œë¡œëŠ” DBë‚˜ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
    suggestions = [
        {"name": "ì‚¼ì„±ì „ì", "code": "005930", "category": "ë°˜ë„ì²´"},
        {"name": "SKí•˜ì´ë‹‰ìŠ¤", "code": "000660", "category": "ë°˜ë„ì²´"},
        {"name": "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "code": "373220", "category": "ë°°í„°ë¦¬"},
        {"name": "í˜„ëŒ€ìë™ì°¨", "code": "005380", "category": "ìë™ì°¨"},
        {"name": "ë„¤ì´ë²„", "code": "035420", "category": "ì¸í„°ë„·"},
        {"name": "ì¹´ì¹´ì˜¤", "code": "035720", "category": "ì¸í„°ë„·"},
        {"name": "ì…€íŠ¸ë¦¬ì˜¨", "code": "068270", "category": "ë°”ì´ì˜¤"},
        {"name": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "code": "207940", "category": "ë°”ì´ì˜¤"}
    ]
    
    return {"suggestions": suggestions}

@router.get("/search")
async def search_news(
    keyword: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    date_from: Optional[str] = Query(None, description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="ì¢…ë£Œì¼ (YYYY-MM-DD)"),
    limit: int = Query(30, description="ê²°ê³¼ ìˆ˜", ge=1, le=100),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ë‰´ìŠ¤ ê²€ìƒ‰ API (GET ë°©ì‹)
    
    í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  íƒ€ì„ë¼ì¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.search")
    logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ìš”ì²­: {keyword}")
    
    try:
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 30ì¼)
        if not date_from:
            date_from = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not date_to:
            date_to = datetime.now().strftime("%Y-%m-%d")
        
        # BigKinds APIë¡œ í‚¤ì›Œë“œ ë‰´ìŠ¤ íƒ€ì„ë¼ì¸ ì¡°íšŒ
        result = bigkinds_client.get_keyword_news_timeline(
            keyword=keyword,
            date_from=date_from,
            date_to=date_to,
            return_size=limit
        )
        
        if not result.get("success", False):
            raise HTTPException(status_code=404, detail="í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "keyword": keyword,
            "period": {
                "from": date_from,
                "to": date_to
            },
            "total_count": result.get("total_count", 0),
            "timeline": result.get("timeline", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.get("/company/{company_name}/summary")
async def get_company_news_summary(
    company_name: str = Path(..., description="ê¸°ì—…ëª…"),
    days: int = Query(7, description="ìµœê·¼ ë©°ì¹ ê°„ì˜ ë‰´ìŠ¤", ge=1, le=30),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ê´€ì‹¬ ì¢…ëª©ì˜ ìµœê·¼ ë‰´ìŠ¤ ìë™ ìš”ì•½
    
    ê¸°ì—…ì˜ ìµœê·¼ ë‰´ìŠ¤ 5ê°œë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì™€ì„œ GPT-4 Turboë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.company_summary")
    logger.info(f"ê¸°ì—… ë‰´ìŠ¤ ìë™ ìš”ì•½ ìš”ì²­: {company_name}")
    
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            raise HTTPException(status_code=500, detail="AI ìš”ì•½ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ê¸°ì—…ì˜ ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        news_data = bigkinds_client.get_company_news_for_summary(
            company_name=company_name,
            days=days,
            limit=1  # ê°œìˆ˜ë§Œ í™•ì¸í•˜ë¯€ë¡œ 1ê°œë§Œ
        )
        
        if not news_data.get("success") or not news_data.get("articles"):
            raise HTTPException(status_code=404, detail=f"{company_name}ì˜ ìµœê·¼ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        articles = news_data.get("articles", [])
        
        # ê¸°ì‚¬ ë‚´ìš© ì¤€ë¹„
        articles_text = ""
        for i, article in enumerate(articles, 1):
            title = article.get("title", "")
            content = article.get("content", "") or article.get("summary", "")
            provider = article.get("provider", "")
            published_at = article.get("published_at", "")
            
            articles_text += f"[ê¸°ì‚¬ {i}]\n"
            articles_text += f"ì œëª©: {title}\n"
            articles_text += f"ì–¸ë¡ ì‚¬: {provider}\n"
            articles_text += f"ë°œí–‰ì¼: {published_at}\n"
            articles_text += f"ë‚´ìš©: {content}\n\n"
        
        # OpenAI GPT-4 Turboë¡œ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (ìƒˆ ë²„ì „ ë°©ì‹)
        try:
            openai_client = get_openai_client()
            response = openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê²½ì œ ë‰´ìŠ¤ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ë“¤ì„ ì¢…í•©í•˜ì—¬ ê°„ê²°í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒì€ '{company_name}' ê´€ë ¨ ìµœê·¼ {days}ì¼ê°„ì˜ ë‰´ìŠ¤ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ì˜ í˜„ì¬ ìƒí™©ê³¼ ì£¼ìš” ì´ìŠˆë¥¼ 300ì ë‚´ì™¸ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{articles_text}"}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            ai_summary = response.choices[0].message.content
        except Exception as e:
            logger.error(f"AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        summary_result = {
            "company": company_name,
            "summary": ai_summary,
            "key_points": [],
            "sentiment": "neutral",
            "investment_implications": "ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "articles_analyzed": len(articles),
            "period": news_data.get("period"),
            "generated_at": datetime.now().isoformat(),
            "model_used": "gpt-4-turbo-preview",
            "source_articles": [
                {
                    "id": article.get("id"),
                    "title": article.get("title"),
                    "published_at": article.get("published_at"),
                    "provider": article.get("provider")
                }
                for article in articles
            ]
        }
        
        return summary_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê¸°ì—… ë‰´ìŠ¤ ìë™ ìš”ì•½ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.get("/company/{company_name}/report/{report_type}")
async def get_company_report(
    company_name: str = Path(..., description="ê¸°ì—…ëª…"),
    report_type: str = Path(..., description="ë ˆí¬íŠ¸ íƒ€ì… (daily, weekly, monthly, quarterly, yearly)"),
    reference_date: Optional[str] = Query(None, description="ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD), ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©"),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ê¸°ì—… ê¸°ê°„ë³„ ë‰´ìŠ¤ ë ˆí¬íŠ¸ ìƒì„±
    
    ê¸°ì—…ëª…ê³¼ ë ˆí¬íŠ¸ íƒ€ì…ì— ë”°ë¼ ê¸°ê°„ë³„ ë‰´ìŠ¤ ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.company_report")
    logger.info(f"ê¸°ì—… ë ˆí¬íŠ¸ ìš”ì²­: {company_name}, íƒ€ì…: {report_type}, ê¸°ì¤€ì¼: {reference_date}")
    
    valid_report_types = ["daily", "weekly", "monthly", "quarterly", "yearly"]
    if report_type not in valid_report_types:
        logger.warning(f"ì˜ëª»ëœ ë ˆí¬íŠ¸ íƒ€ì… ìš”ì²­: {report_type}")
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë ˆí¬íŠ¸ íƒ€ì…ì…ë‹ˆë‹¤. ìœ íš¨í•œ íƒ€ì…: {', '.join(valid_report_types)}")
    
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            logger.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return {
                "success": False,
                "error": "AI ìš”ì•½ ì„œë¹„ìŠ¤ì— í•„ìš”í•œ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                "company": company_name,
                "report_type": report_type,
                "report_type_kr": get_report_type_kr(report_type),
                "message": "ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            }
        
        # BigKinds API í‚¤ í™•ì¸
        if not os.getenv("BIGKINDS_KEY"):
            logger.error("BIGKINDS_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return {
                "success": False,
                "error": "ë‰´ìŠ¤ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— í•„ìš”í•œ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                "company": company_name,
                "report_type": report_type,
                "report_type_kr": get_report_type_kr(report_type),
                "message": "ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            }
        
        # API í‚¤ ë¡œê¹… (ë§ˆìŠ¤í‚¹ ì²˜ë¦¬)
        openai_key_status = "ì„¤ì •ë¨" if os.getenv("OPENAI_API_KEY") else "ì„¤ì •ë˜ì§€ ì•ŠìŒ"
        bigkinds_key_status = "ì„¤ì •ë¨" if os.getenv("BIGKINDS_KEY") else "ì„¤ì •ë˜ì§€ ì•ŠìŒ"
        logger.info(f"API í‚¤ ìƒíƒœ - OpenAI: {openai_key_status}, BigKinds: {bigkinds_key_status}")
        
        # BigKinds APIë¡œ ê¸°ì—… ë‰´ìŠ¤ ë ˆí¬íŠ¸ ë°ì´í„° ì¡°íšŒ
        logger.info(f"ê¸°ì—… ë‰´ìŠ¤ ë ˆí¬íŠ¸ ë°ì´í„° ì¡°íšŒ ì‹œì‘: {company_name}")
        report_data = bigkinds_client.get_company_news_report(
            company_name=company_name,
            report_type=report_type,
            reference_date=reference_date
        )
        
        if not report_data.get("success", False):
            logger.warning(f"ê¸°ì—… ë‰´ìŠ¤ ë ˆí¬íŠ¸ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {report_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            raise HTTPException(status_code=404, detail=f"ê¸°ì—… ë‰´ìŠ¤ ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        articles = report_data.get("articles", [])
        logger.info(f"ì¡°íšŒëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")
        
        if not articles:
            logger.warning(f"ì¡°íšŒëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤: {company_name}, {report_type}")
            return {
                **report_data,
                "summary": f"{company_name}ì— ëŒ€í•œ {report_data.get('report_type_kr')} ë ˆí¬íŠ¸ ê¸°ê°„ ë‚´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ê° ê¸°ì‚¬ì— ê³ ìœ  ID ë¶€ì—¬ (ì¸ìš© ì°¸ì¡°ìš©)
        for i, article in enumerate(articles):
            if not article.get("ref_id"):
                article["ref_id"] = f"ref{i+1}"
        
        # ê¸°ì‚¬ ë‚´ìš© ì¤€ë¹„ (ì „ì²´ content ì‚¬ìš©)
        articles_text = ""
        for i, article in enumerate(articles, 1):
            ref_id = article.get("ref_id", f"ref{i}")
            title = article.get("title", "")
            content = article.get("content", "") or article.get("summary", "")
            provider = article.get("provider", "")
            published_at = article.get("published_at", "")
            
            articles_text += f"[ê¸°ì‚¬ {ref_id}]\n"
            articles_text += f"ì œëª©: {title}\n"
            articles_text += f"ì–¸ë¡ ì‚¬: {provider}\n"
            articles_text += f"ë°œí–‰ì¼: {published_at}\n"
            articles_text += f"ë‚´ìš©: {content}\n\n"
        
        # í† í° í•œê³„ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì²­í‚¹ ë° ìš”ì•½ ì²˜ë¦¬
        max_tokens = 4000  # ì²­í¬ë‹¹ ìµœëŒ€ í† í° ìˆ˜ (ëŒ€ëµì ì¸ ì¶”ì •)
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for i, article in enumerate(articles, 1):
            ref_id = article.get("ref_id", f"ref{i}")
            article_text = f"[ê¸°ì‚¬ {ref_id}]\n"
            article_text += f"ì œëª©: {article.get('title', '')}\n"
            article_text += f"ë‚´ìš©: {article.get('content', '') or article.get('summary', '')}\n\n"
            
            # ëŒ€ëµì ìœ¼ë¡œ í† í° ìˆ˜ ì¶”ì • (ì˜ì–´ ê¸°ì¤€ 4ìë‹¹ 1í† í°, í•œê¸€ì€ ë” ì ì„ ìˆ˜ ìˆìŒ)
            article_tokens = len(article_text) // 2
            
            if current_tokens + article_tokens > max_tokens:
                chunks.append(current_chunk)
                current_chunk = article_text
                current_tokens = article_tokens
            else:
                current_chunk += article_text
                current_tokens += article_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        logger.info(f"ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        
        # ê¸°ê°„ì— ë”°ë¥¸ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        period_from = report_data["period"]["from"]
        period_to = report_data["period"]["to"]
        
        prompts = {
            "daily": f"{period_to}ì¼ í•˜ë£¨ ë™ì•ˆì˜ {company_name} ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "weekly": f"{period_from}ë¶€í„° {period_to}ê¹Œì§€ ì¼ì£¼ì¼ ê°„ì˜ {company_name} ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ì™€ ë™í–¥ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "monthly": f"{period_from}ë¶€í„° {period_to}ê¹Œì§€ í•œ ë‹¬ ê°„ì˜ {company_name}ì˜ ì£¼ìš” ì´ìŠˆ, ë™í–¥ ë° ë³€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "quarterly": f"{period_from}ë¶€í„° {period_to}ê¹Œì§€ 3ê°œì›” ê°„ì˜ {company_name}ì˜ ë¶„ê¸°ë³„ ì„±ê³¼, ì£¼ìš” ì´ìŠˆ ë° ë³€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "yearly": f"{period_from}ë¶€í„° {period_to}ê¹Œì§€ 1ë…„ ê°„ì˜ {company_name}ì˜ ì£¼ìš” ì´ìŠˆ, ì„±ê³¼, ì‹œì¥ ë³€í™” ë° ì „ëµì  ë°©í–¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”."
        }
        
        # ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ì¸ìš© ì •ë³´ í¬í•¨ ìš”ì²­
        system_prompt = """ë‹¹ì‹ ì€ ê¸ˆìœµ ë° ê²½ì œ ë¶„ì•¼ì˜ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ê°ê´€ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì¤‘ìš”í•œ ì‚¬ì‹¤ì´ë‚˜ ì£¼ì¥ì„ ì¸ìš©í•  ë•Œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”. ì˜ˆ: [ê¸°ì‚¬ ref1]
2. ì§ì ‘ ì¸ìš©êµ¬ëŠ” í°ë”°ì˜´í‘œë¡œ í‘œì‹œí•˜ê³  ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”. ì˜ˆ: "ì‚¼ì„±ì „ìëŠ” ì‹ ê·œ íˆ¬ìë¥¼ ë°œí‘œí–ˆë‹¤"[ê¸°ì‚¬ ref2]
3. ìš”ì•½ì€ ì£¼ìš” ì´ìŠˆ, ë™í–¥, ì˜í–¥ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.
4. ìš”ì•½ ë§ë¯¸ì— ëª¨ë“  ì°¸ê³  ê¸°ì‚¬ ëª©ë¡ì„ í¬í•¨í•˜ì„¸ìš”.

ì´ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        user_prompt = prompts.get(report_type, prompts["daily"]) + "\n\nê° ê¸°ì‚¬ì˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” [ê¸°ì‚¬ refë²ˆí˜¸] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”."
        
        logger.info("ìš”ì•½ ìƒì„± ì‹œì‘")
        
        # ì²­í¬ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ê°ê° ìš”ì•½ í›„ ë©”íƒ€ ìš”ì•½
        final_summary = ""
        if len(chunks) > 1:
            chunk_summaries = []
            
            for i, chunk in enumerate(chunks, 1):
                try:
                    logger.info(f"ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ìƒì„± ì¤‘...")
                    openai_client = get_openai_client()
                    chunk_response = openai_client.chat.completions.create(
                        model="gpt-4-turbo-preview",
                        messages=[
                            {"role": "system", "content": "ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” [ê¸°ì‚¬ refë²ˆí˜¸] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”."},
                            {"role": "user", "content": f"ë‹¤ìŒì€ {company_name} ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” [ê¸°ì‚¬ refë²ˆí˜¸] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”:\n\n{chunk}"}
                        ],
                        max_tokens=800,
                        temperature=0.3
                    )
                    
                    chunk_summary = chunk_response.choices[0].message.content
                    chunk_summaries.append(f"íŒŒíŠ¸ {i} ìš”ì•½: {chunk_summary}")
                    logger.info(f"ì²­í¬ {i} ìš”ì•½ ì™„ë£Œ (ê¸¸ì´: {len(chunk_summary)}ì)")
                except Exception as e:
                    logger.error(f"ì²­í¬ {i} ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
                    chunk_summaries.append(f"íŒŒíŠ¸ {i} ìš”ì•½: ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
            
            # ë©”íƒ€ ìš”ì•½ ìƒì„±
            meta_summaries_text = "\n\n".join(chunk_summaries)
            try:
                logger.info("ë©”íƒ€ ìš”ì•½ ìƒì„± ì¤‘...")
                openai_client = get_openai_client()
                meta_response = openai_client.chat.completions.create(
                    model="gpt-4-turbo-preview",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"{user_prompt}\n\në‹¤ìŒì€ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ ì§„ ìš”ì•½ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° ë¶€ë¶„ì— ìˆëŠ” ì¸ìš© ì •ë³´([ê¸°ì‚¬ refë²ˆí˜¸])ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”:\n\n{meta_summaries_text}"}
                    ],
                    max_tokens=1500,
                    temperature=0.3
                )
                
                final_summary = meta_response.choices[0].message.content
                logger.info(f"ë©”íƒ€ ìš”ì•½ ì™„ë£Œ (ê¸¸ì´: {len(final_summary)}ì)")
            except Exception as e:
                logger.error(f"ë©”íƒ€ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
                final_summary = "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        else:
            # ë‹¨ì¼ ì²­í¬ì¼ ê²½ìš° ë°”ë¡œ ìš”ì•½
            try:
                logger.info("ë‹¨ì¼ ì²­í¬ ìš”ì•½ ìƒì„± ì¤‘...")
                openai_client = get_openai_client()
                response = openai_client.chat.completions.create(
                    model="gpt-4-turbo-preview",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"{user_prompt}\n\në‹¤ìŒì€ {company_name} ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. ê° ê¸°ì‚¬ì˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” [ê¸°ì‚¬ refë²ˆí˜¸] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”:\n\n{articles_text}"}
                    ],
                    max_tokens=1500,
                    temperature=0.3
                )
                
                final_summary = response.choices[0].message.content
                logger.info(f"ìš”ì•½ ì™„ë£Œ (ê¸¸ì´: {len(final_summary)}ì)")
            except Exception as e:
                logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
                final_summary = "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ëª¨ë“  ê¸°ì‚¬ ì •ë³´ í¬í•¨ (content í•„ë“œë¥¼ ì œì™¸í•˜ê³  ê¸°ë³¸ ì •ë³´ë§Œ í¬í•¨)
        detailed_articles = []
        for article in articles:
            detailed_articles.append({
                "id": article.get("id", ""),
                "ref_id": article.get("ref_id", ""),
                "title": article.get("title", ""),
                "summary": article.get("summary", ""),
                "provider": article.get("provider", ""),
                "published_at": article.get("published_at", ""),
                "url": article.get("url", ""),
                "category": article.get("category", ""),
                "byline": article.get("byline", "")
            })
        
        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        logger.info(f"ë ˆí¬íŠ¸ ìƒì„± ì™„ë£Œ: {company_name}, {report_type}")
        return {
            **report_data,
            "summary": final_summary,
            "detailed_articles": detailed_articles,  # ëª¨ë“  ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ í¬í•¨
            "generated_at": datetime.now().isoformat(),
            "model_used": "gpt-4-turbo-preview"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê¸°ì—… ë ˆí¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
        # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ë¥¼ í¬í•¨í•œ ì‘ë‹µ
        return {
            "success": False,
            "error": f"ê¸°ì—… ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "company": company_name,
            "report_type": report_type,
            "report_type_kr": get_report_type_kr(report_type),
            "generated_at": datetime.now().isoformat()
        }

@router.get("/watchlist")
async def get_watchlist_data(
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ê´€ì‹¬ ì¢…ëª© ì „ì²´ ë°ì´í„° (ì¢…ëª© ëª©ë¡ + ê° ì¢…ëª©ë³„ ìµœì‹  ë‰´ìŠ¤ ìˆ˜)
    
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ê´€ì‹¬ ì¢…ëª© ì„¹ì…˜ì„ êµ¬ì„±í•  ë•Œ í•„ìš”í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.watchlist")
    logger.info("ê´€ì‹¬ ì¢…ëª© ë°ì´í„° ìš”ì²­")
    
    try:
        # ì¶”ì²œ ì¢…ëª© ëª©ë¡ (í•˜ë“œì½”ë”©ëœ ì£¼ìš” ê¸°ì—…ë“¤)
        watchlist_companies = [
            {"name": "ì‚¼ì„±ì „ì", "code": "005930", "category": "ë°˜ë„ì²´"},
            {"name": "SKí•˜ì´ë‹‰ìŠ¤", "code": "000660", "category": "ë°˜ë„ì²´"},
            {"name": "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "code": "373220", "category": "ë°°í„°ë¦¬"},
            {"name": "í˜„ëŒ€ìë™ì°¨", "code": "005380", "category": "ìë™ì°¨"},
            {"name": "ë„¤ì´ë²„", "code": "035420", "category": "ì¸í„°ë„·"},
            {"name": "ì¹´ì¹´ì˜¤", "code": "035720", "category": "ì¸í„°ë„·"},
            {"name": "ì…€íŠ¸ë¦¬ì˜¨", "code": "068270", "category": "ë°”ì´ì˜¤"},
            {"name": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "code": "207940", "category": "ë°”ì´ì˜¤"}
        ]
        
        # ê° ê¸°ì—…ë³„ ìµœê·¼ ë‰´ìŠ¤ ìˆ˜ í™•ì¸ (ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ)
        enhanced_watchlist = []
        for company in watchlist_companies:
            try:
                # ìµœê·¼ 7ì¼ê°„ ë‰´ìŠ¤ ìˆ˜ í™•ì¸
                news_data = bigkinds_client.get_company_news_for_summary(
                    company_name=company["name"],
                    days=7,
                    limit=1  # ê°œìˆ˜ë§Œ í™•ì¸í•˜ë¯€ë¡œ 1ê°œë§Œ
                )
                
                company_data = {
                    **company,
                    "recent_news_count": news_data.get("total_found", 0),
                    "has_recent_news": news_data.get("total_found", 0) > 0,
                    "last_updated": datetime.now().isoformat()
                }
                enhanced_watchlist.append(company_data)
                
            except Exception as e:
                logger.warning(f"ê¸°ì—… {company['name']} ë‰´ìŠ¤ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                company_data = {
                    **company,
                    "recent_news_count": 0,
                    "has_recent_news": False,
                    "last_updated": datetime.now().isoformat()
                }
                enhanced_watchlist.append(company_data)
        
        return {
            "success": True,
            "watchlist": enhanced_watchlist,
            "total_companies": len(enhanced_watchlist),
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ê´€ì‹¬ ì¢…ëª© ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
        return {
            "success": False,
            "watchlist": [
                {"name": "ì‚¼ì„±ì „ì", "code": "005930", "category": "ë°˜ë„ì²´", "recent_news_count": 0, "has_recent_news": False},
                {"name": "SKí•˜ì´ë‹‰ìŠ¤", "code": "000660", "category": "ë°˜ë„ì²´", "recent_news_count": 0, "has_recent_news": False},
                {"name": "í˜„ëŒ€ìë™ì°¨", "code": "005380", "category": "ìë™ì°¨", "recent_news_count": 0, "has_recent_news": False}
            ],
            "total_companies": 3,
            "generated_at": datetime.now().isoformat(),
            "error": str(e)
        }

@router.get("/related-questions/{keyword}")
async def get_related_questions(
    keyword: str = Path(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    days: Optional[int] = Query(None, description="ê²€ìƒ‰ ê¸°ê°„(ì¼)", ge=1, le=365),
    date_from: Optional[str] = Query(None, description="ì‹œì‘ì¼(YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="ì¢…ë£Œì¼(YYYY-MM-DD)"),
    max_questions: int = Query(7, description="ìµœëŒ€ ì§ˆë¬¸ ìˆ˜", ge=1, le=20),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì—°ê´€ ì§ˆë¬¸ ìƒì„±
    
    í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—°ê´€ ê²€ìƒ‰ì–´ì™€ TopN í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ì—°ê´€ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    from backend.api.utils.keywords_utils import (
        filter_keywords, score_keywords, create_boolean_queries, 
        keywords_to_questions, get_topic_sensitive_date_range
    )
    
    logger = setup_logger("api.news.related_questions")
    logger.info(f"ì—°ê´€ ì§ˆë¬¸ ìš”ì²­: {keyword}")
    
    try:
        # ì£¼ì œ ê¸°ë°˜ ê¸°ê°„ ì„¤ì •
        topic_date_range = get_topic_sensitive_date_range(keyword)
        
        # ì‚¬ìš©ì ì§€ì • ê¸°ê°„ ì²˜ë¦¬
        if date_from and date_to:
            period = {"date_from": date_from, "date_to": date_to}
        elif days:
            # ì¼ìˆ˜ë¡œ ê¸°ê°„ ì§€ì •
            date_to = datetime.now().strftime("%Y-%m-%d")
            date_from = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            period = {"date_from": date_from, "date_to": date_to}
        else:
            # ì£¼ì œ ê¸°ë°˜ ê¸°ê°„ ì‚¬ìš©
            period = {
                "date_from": topic_date_range["date_from"],
                "date_to": topic_date_range["date_to"]
            }
        
        # 1ë‹¨ê³„: ì—°ê´€ì–´ì™€ TopN í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        related_keywords = bigkinds_client.get_related_keywords(
            keyword=keyword, 
            max_count=20,
            date_from=period["date_from"],
            date_to=period["date_to"]
        )
        
        topn_keywords = bigkinds_client.get_keyword_topn(
            keyword=keyword,
            date_from=period["date_from"],
            date_to=period["date_to"]
        )
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ í•„í„°ë§ ë° ì ìˆ˜í™”
        filtered_related = filter_keywords(related_keywords)
        filtered_topn = filter_keywords(topn_keywords)
        
        # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê¸°ê°„ í™•ì¥ ì‹œë„
        if len(filtered_related) < 5 or len(filtered_topn) < 5:
            # ê¸°ê°„ í™•ì¥ (60ì¼)
            extended_date_from = (datetime.now() - timedelta(days=60)).strftime("%Y-%m-%d")
            
            # í™•ì¥ëœ ê¸°ê°„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
            extended_related = bigkinds_client.get_related_keywords(
                keyword=keyword, 
                max_count=30,
                date_from=extended_date_from,
                date_to=period["date_to"]
            )
            
            extended_topn = bigkinds_client.get_keyword_topn(
                keyword=keyword,
                date_from=extended_date_from,
                date_to=period["date_to"],
                limit=30
            )
            
            # í™•ì¥ëœ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ìœ ì§€)
            if len(filtered_related) < 5 and len(extended_related) > len(filtered_related):
                filtered_related = filter_keywords(extended_related)
                
            if len(filtered_topn) < 5 and len(extended_topn) > len(filtered_topn):
                filtered_topn = filter_keywords(extended_topn)
                
            # ê¸°ê°„ ì •ë³´ ì—…ë°ì´íŠ¸
            period["date_from"] = extended_date_from
        
        # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        keyword_scores = score_keywords(keyword, filtered_related, filtered_topn)
        
        # 3ë‹¨ê³„: ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        sorted_keywords = sorted(keyword_scores.keys(), key=lambda k: keyword_scores[k], reverse=True)
        top_keywords = sorted_keywords[:15]  # ìƒìœ„ 15ê°œë§Œ ì‚¬ìš©
        
        # 4ë‹¨ê³„: ì¿¼ë¦¬ ë³€í˜• ìƒì„±
        query_variations = create_boolean_queries(keyword, top_keywords, max_variations=10)
        
        # 5ë‹¨ê³„: ì§ˆë¬¸ ìƒì„±
        questions = keywords_to_questions(keyword, query_variations)
        
        # ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ë¡œ ì œí•œ
        limited_questions = questions[:max_questions]
        
        return {
            "success": True,
            "keyword": keyword,
            "period": {
                "from": period["date_from"],
                "to": period["date_to"]
            },
            "detected_topic": topic_date_range.get("detected_topic"),
            "questions": limited_questions,
            "total_questions": len(limited_questions),
            "keywords": {
                "related": filtered_related[:10],
                "topn": filtered_topn[:10],
                "top_scored": top_keywords[:10]
            },
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ì—°ê´€ ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "success": False,
            "keyword": keyword,
            "error": str(e),
            "questions": [],
            "generated_at": datetime.now().isoformat()
        }

@router.post("/search/news")
async def search_news_content(
    keyword: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    date_from: Optional[str] = Query(None, description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="ì¢…ë£Œì¼ (YYYY-MM-DD)"),
    limit: int = Query(30, description="ê²°ê³¼ ìˆ˜", ge=1, le=100),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ë‰´ìŠ¤ ì „ì²´ ë‚´ìš© ê²€ìƒ‰ API
    
    í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ì „ì²´ ë‚´ìš©(content)ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.search_content")
    logger.info(f"ë‰´ìŠ¤ ë‚´ìš© ê²€ìƒ‰ ìš”ì²­: {keyword}")
    
    try:
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 30ì¼)
        if not date_from:
            date_from = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not date_to:
            # untilì€ ì˜¤ëŠ˜ ë‚ ì§œ +1ì¼ë¡œ ì„¤ì • (ì˜¤ëŠ˜ ë°ì´í„° í¬í•¨ ìœ„í•´)
            date_to = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
        
        # í•„ìˆ˜ í•„ë“œ ì„¤ì • (content í¬í•¨)
        fields = [
            "news_id",
            "title",
            "content",  # ì „ì²´ ë‚´ìš© í¬í•¨
            "published_at",
            "provider_name",
            "provider_code",
            "provider_link_page",
            "byline",
            "category",
            "images"
        ]
        
        # BigKinds APIë¡œ ì§ì ‘ ë‰´ìŠ¤ ê²€ìƒ‰ (íƒ€ì„ë¼ì¸ ì•„ë‹Œ ì›ë³¸ ë°ì´í„°)
        result = bigkinds_client.search_news(
            query=keyword,
            date_from=date_from,
            date_to=date_to,
            return_size=limit,
            fields=fields
        )
        
        # ì‘ë‹µ í¬ë§·íŒ…
        formatted_result = bigkinds_client.format_news_response(result)
        
        if not formatted_result.get("success", False):
            raise HTTPException(status_code=404, detail="í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "keyword": keyword,
            "period": {
                "from": date_from,
                "to": date_to
            },
            "total_count": formatted_result.get("total_hits", 0),
            "articles": formatted_result.get("documents", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë‚´ìš© ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ë‚´ìš© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.get("/search/news")
async def search_news_content_get(
    keyword: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    date_from: Optional[str] = Query(None, description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="ì¢…ë£Œì¼ (YYYY-MM-DD)"),
    limit: int = Query(30, description="ê²°ê³¼ ìˆ˜", ge=1, le=100),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ë‰´ìŠ¤ ì „ì²´ ë‚´ìš© ê²€ìƒ‰ API (GET ë°©ì‹)
    
    í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ì „ì²´ ë‚´ìš©(content)ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.search_content_get")
    logger.info(f"ë‰´ìŠ¤ ë‚´ìš© ê²€ìƒ‰ ìš”ì²­(GET): {keyword}")
    
    try:
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 30ì¼)
        if not date_from:
            date_from = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not date_to:
            # untilì€ ì˜¤ëŠ˜ ë‚ ì§œ +1ì¼ë¡œ ì„¤ì • (ì˜¤ëŠ˜ ë°ì´í„° í¬í•¨ ìœ„í•´)
            date_to = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
        
        # í•„ìˆ˜ í•„ë“œ ì„¤ì • (content í¬í•¨)
        fields = [
            "news_id",
            "title",
            "content",  # ì „ì²´ ë‚´ìš© í¬í•¨
            "published_at",
            "provider_name",
            "provider_code",
            "provider_link_page",
            "byline",
            "category",
            "images"
        ]
        
        # BigKinds APIë¡œ ì§ì ‘ ë‰´ìŠ¤ ê²€ìƒ‰ (íƒ€ì„ë¼ì¸ ì•„ë‹Œ ì›ë³¸ ë°ì´í„°)
        result = bigkinds_client.search_news(
            query=keyword,
            date_from=date_from,
            date_to=date_to,
            return_size=limit,
            fields=fields
        )
        
        # ì‘ë‹µ í¬ë§·íŒ…
        formatted_result = bigkinds_client.format_news_response(result)
        
        if not formatted_result.get("success", False):
            raise HTTPException(status_code=404, detail="í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "keyword": keyword,
            "period": {
                "from": date_from,
                "to": date_to
            },
            "total_count": formatted_result.get("total_hits", 0),
            "articles": formatted_result.get("documents", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë‚´ìš© ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ë‚´ìš© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.get("/search-by-question")
async def search_by_question(
    query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬ (ë¶ˆë¦¬ì–¸ ì—°ì‚°ì ì§€ì›)"),
    question: Optional[str] = Query(None, description="ì›ë˜ ì§ˆë¬¸ (í‘œì‹œìš©)"),
    date_from: Optional[str] = Query(None, description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="ì¢…ë£Œì¼ (YYYY-MM-DD)"),
    limit: int = Query(10, description="ê²°ê³¼ ìˆ˜", ge=1, le=100),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ì§ˆë¬¸ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
    
    ì—°ê´€ ì§ˆë¬¸ì—ì„œ ìƒì„±ëœ ë¶ˆë¦¬ì–¸ ì¿¼ë¦¬ë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.search_by_question")
    logger.info(f"ì§ˆë¬¸ ê²€ìƒ‰ ìš”ì²­: '{query}', ì§ˆë¬¸: '{question}'")
    
    try:
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 30ì¼)
        if not date_from:
            date_from = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not date_to:
            date_to = datetime.now().strftime("%Y-%m-%d")
        
        # í•„ìˆ˜ í•„ë“œ ì„¤ì • (content í¬í•¨)
        fields = [
            "news_id",
            "title",
            "content",  # ì „ì²´ ë‚´ìš© í¬í•¨
            "summary",
            "published_at",
            "provider_name",
            "provider_code",
            "provider_link_page",
            "byline",
            "category",
            "images"
        ]
        
        # BigKinds APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
        result = bigkinds_client.search_news(
            query=query,
            date_from=date_from,
            date_to=date_to,
            return_size=limit,
            fields=fields
        )
        
        # ì‘ë‹µ í¬ë§·íŒ…
        formatted_result = bigkinds_client.format_news_response(result)
        
        if not formatted_result.get("success", False):
            raise HTTPException(status_code=404, detail="ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "original_query": query,
            "question": question or "ì§ˆë¬¸ ì •ë³´ ì—†ìŒ",
            "period": {
                "from": date_from,
                "to": date_to
            },
            "total_count": formatted_result.get("total_hits", 0),
            "articles": formatted_result.get("documents", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.post("/watchlist", response_model=WatchlistResponse)
async def add_to_watchlist(
    request: WatchlistAddRequest,
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ê´€ì‹¬ì¢…ëª©ì— ê¸°ì—… ì¶”ê°€
    
    ì‚¬ìš©ìì˜ ê´€ì‹¬ì¢…ëª© ëª©ë¡ì— ìƒˆë¡œìš´ ê¸°ì—…ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.watchlist_add")
    logger.info(f"ê´€ì‹¬ì¢…ëª© ì¶”ê°€ ìš”ì²­: {request.name} ({request.code})")
    
    try:
        # TODO: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        # í˜„ì¬ëŠ” ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„ì‹œ êµ¬í˜„
        
        # ê¸°ì—… ì •ë³´ ê²€ì¦ (BigKinds APIë¡œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê¸°ì—…ì¸ì§€ í™•ì¸)
        try:
            # ìµœê·¼ 7ì¼ê°„ ë‰´ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ìœ íš¨í•œ ê¸°ì—…ì¸ì§€ ê²€ì¦
            news_data = bigkinds_client.get_company_news_for_summary(
                company_name=request.name,
                days=30,  # 30ì¼ë¡œ í™•ì¥í•˜ì—¬ ê²€ì¦
                limit=1
            )
            
            # ë‰´ìŠ¤ê°€ ì „í˜€ ì—†ìœ¼ë©´ ì˜ëª»ëœ ê¸°ì—…ëª…ì¼ ê°€ëŠ¥ì„±
            if news_data.get("total_found", 0) == 0:
                logger.warning(f"ê¸°ì—… '{request.name}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return WatchlistResponse(
                    success=False,
                    message=f"'{request.name}' ê¸°ì—…ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì—…ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
                
        except Exception as e:
            logger.warning(f"ê¸°ì—… ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì¶”ê°€ëŠ” í—ˆìš© (API ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŒ)
        
        # ê´€ì‹¬ì¢…ëª© ëª©ë¡ ë¡œë“œ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì‚¬ìš©ìë³„ë¡œ ê´€ë¦¬)
        # í˜„ì¬ëŠ” ì„¸ì…˜ ê¸°ë°˜ ì„ì‹œ êµ¬í˜„
        watchlist_key = "user_watchlist"  # ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì ID ê¸°ë°˜
        
        # ì„ì‹œ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” Redisë‚˜ DB ì‚¬ìš©)
        if not hasattr(add_to_watchlist, '_watchlist_storage'):
            add_to_watchlist._watchlist_storage = {}
        
        user_watchlist = add_to_watchlist._watchlist_storage.get(watchlist_key, [])
        
        # ì¤‘ë³µ í™•ì¸
        existing_item = next((item for item in user_watchlist if item["code"] == request.code), None)
        if existing_item:
            return WatchlistResponse(
                success=False,
                message=f"'{request.name}' ê¸°ì—…ì´ ì´ë¯¸ ê´€ì‹¬ì¢…ëª©ì— ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            )
        
        # ìƒˆ í•­ëª© ì¶”ê°€
        new_item = {
            "name": request.name,
            "code": request.code,
            "category": request.category,
            "added_at": datetime.now().isoformat(),
            "recent_news_count": 0,
            "has_recent_news": False
        }
        
        user_watchlist.append(new_item)
        add_to_watchlist._watchlist_storage[watchlist_key] = user_watchlist
        
        logger.info(f"ê´€ì‹¬ì¢…ëª© ì¶”ê°€ ì™„ë£Œ: {request.name} ({request.code})")
        
        return WatchlistResponse(
            success=True,
            message=f"'{request.name}' ê¸°ì—…ì´ ê´€ì‹¬ì¢…ëª©ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            watchlist=user_watchlist
        )
        
    except Exception as e:
        logger.error(f"ê´€ì‹¬ì¢…ëª© ì¶”ê°€ ì˜¤ë¥˜: {e}", exc_info=True)
        return WatchlistResponse(
            success=False,
            message=f"ê´€ì‹¬ì¢…ëª© ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.delete("/watchlist/{stock_code}", response_model=WatchlistResponse)
async def remove_from_watchlist(
    stock_code: str = Path(..., description="ì‚­ì œí•  ì¢…ëª©ì½”ë“œ"),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ê´€ì‹¬ì¢…ëª©ì—ì„œ ì œê±°
    
    íŠ¹ì • ì¢…ëª©ì„ ê´€ì‹¬ì¢…ëª© ëª©ë¡ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.remove_watchlist")
    logger.info(f"ê´€ì‹¬ì¢…ëª© ì œê±° ìš”ì²­: {stock_code}")
    
    try:
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œê±°í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤
        # í˜„ì¬ëŠ” ë”ë¯¸ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤
        
        return WatchlistResponse(
            success=True,
            message=f"ì¢…ëª©ì½”ë“œ {stock_code}ì´(ê°€) ê´€ì‹¬ì¢…ëª©ì—ì„œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.",
            watchlist=[]  # ì—…ë°ì´íŠ¸ëœ ê´€ì‹¬ì¢…ëª© ëª©ë¡ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤
        )
        
    except Exception as e:
        logger.error(f"ê´€ì‹¬ì¢…ëª© ì œê±° ì˜¤ë¥˜: {e}", exc_info=True)
        return WatchlistResponse(
            success=False,
            message=f"ê´€ì‹¬ì¢…ëª© ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.post("/search-by-hours")
async def search_news_by_hours(
    keyword: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    hours: int = Query(default=24, description="ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„)", ge=1, le=168),
    limit: int = Query(default=20, description="ê²°ê³¼ ìˆ˜", ge=1, le=100),
    bigkinds_client: BigKindsClient = Depends(get_bigkinds_client)
):
    """ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ ê²€ìƒ‰
    
    ì§€ì •ëœ ì‹œê°„ ë²”ìœ„ ë‚´ì˜ í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.search_by_hours")
    logger.info(f"ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ ê²€ìƒ‰ ìš”ì²­: {keyword}, {hours}ì‹œê°„")
    
    try:
        # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        date_from = start_time.strftime("%Y-%m-%d")
        date_to = end_time.strftime("%Y-%m-%d")
        
        # BigKinds APIë¡œ í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰
        result = bigkinds_client.get_keyword_news_timeline(
            keyword=keyword,
            date_from=date_from,
            date_to=date_to,
            return_size=limit
        )
        
        if not result.get("success", False):
            return {
                "success": False,
                "keyword": keyword,
                "hours": hours,
                "total_count": 0,
                "articles": [],
                "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
        
        return {
            "success": True,
            "keyword": keyword,
            "hours": hours,
            "period": {
                "from": date_from,
                "to": date_to
            },
            "total_count": result.get("total_count", 0),
            "timeline": result.get("timeline", []),
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "success": False,
            "keyword": keyword,
            "hours": hours,
            "total_count": 0,
            "articles": [],
            "error": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

@router.get("/categories")
async def get_news_categories():
    """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ
    
    ì‚¬ìš© ê°€ëŠ¥í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.categories")
    logger.info("ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ìš”ì²­")
    
    try:
        # ë¹…ì¹´ì¸ì¦ˆì—ì„œ ì§€ì›í•˜ëŠ” ì£¼ìš” ì¹´í…Œê³ ë¦¬ë“¤
        categories = [
            {
                "id": "politics",
                "name": "ì •ì¹˜",
                "description": "ì •ì¹˜ ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            },
            {
                "id": "economy",
                "name": "ê²½ì œ",
                "description": "ê²½ì œ ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            },
            {
                "id": "society",
                "name": "ì‚¬íšŒ",
                "description": "ì‚¬íšŒ ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            },
            {
                "id": "culture",
                "name": "ë¬¸í™”",
                "description": "ë¬¸í™” ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            },
            {
                "id": "international",
                "name": "êµ­ì œ",
                "description": "êµ­ì œ ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            },
            {
                "id": "sports",
                "name": "ìŠ¤í¬ì¸ ",
                "description": "ìŠ¤í¬ì¸  ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            },
            {
                "id": "it_science",
                "name": "IT/ê³¼í•™",
                "description": "IT ë° ê³¼í•™ ê´€ë ¨ ë‰´ìŠ¤",
                "count": 0
            }
        ]
        
        return {
            "success": True,
            "categories": categories,
            "total_categories": len(categories),
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "success": False,
            "categories": [],
            "total_categories": 0,
            "error": f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

@router.get("/stats")
async def get_news_stats():
    """ë‰´ìŠ¤ í†µê³„ ì •ë³´ ì¡°íšŒ
    
    ì „ì²´ ë‰´ìŠ¤ í†µê³„ ë° íŠ¸ë Œë“œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.stats")
    logger.info("ë‰´ìŠ¤ í†µê³„ ì •ë³´ ìš”ì²­")
    
    try:
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ í†µê³„ ì •ë³´ (ë”ë¯¸ ë°ì´í„°)
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        week_ago = today - timedelta(days=7)
        
        stats = {
            "daily_stats": {
                "today": {
                    "date": today.strftime("%Y-%m-%d"),
                    "total_articles": 0,  # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œë¡œ ê°€ì ¸ì™€ì•¼ í•¨
                    "categories": {
                        "politics": 0,
                        "economy": 0,
                        "society": 0,
                        "culture": 0,
                        "international": 0,
                        "sports": 0,
                        "it_science": 0
                    }
                },
                "yesterday": {
                    "date": yesterday.strftime("%Y-%m-%d"),
                    "total_articles": 0,
                    "categories": {
                        "politics": 0,
                        "economy": 0,
                        "society": 0,
                        "culture": 0,
                        "international": 0,
                        "sports": 0,
                        "it_science": 0
                    }
                }
            },
            "weekly_stats": {
                "period": {
                    "from": week_ago.strftime("%Y-%m-%d"),
                    "to": today.strftime("%Y-%m-%d")
                },
                "total_articles": 0,
                "daily_breakdown": [],
                "top_keywords": [],
                "trending_topics": []
            },
            "system_stats": {
                "total_indexed_articles": 0,
                "last_updated": datetime.now().isoformat(),
                "api_status": "active",
                "data_sources": ["BigKinds"]
            }
        }
        
        return {
            "success": True,
            "stats": stats,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "success": False,
            "stats": {},
            "error": f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

# AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì„í¬íŠ¸ ì¶”ê°€
from backend.services.news_concierge import (
    NewsConciergeService, ConciergeRequest, ConciergeResponse, ConciergeProgress
)

# ì»¨ì‹œì–´ì§€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„±
def get_concierge_service() -> NewsConciergeService:
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„±"""
    from backend.api.dependencies import get_news_concierge_service
    return get_news_concierge_service()

@router.post("/concierge", response_model=ConciergeResponse)
async def get_concierge_response(
    request: ConciergeRequest,
    concierge_service: NewsConciergeService = Depends(get_concierge_service)
):
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„±
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIê°€ ìƒì„±í•œ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    ê°ì£¼ ì‹œìŠ¤í…œê³¼ ì—°ê´€ì–´, ì˜¤ëŠ˜ì˜ ì´ìŠˆë¥¼ í¬í•¨í•œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.concierge")
    logger.info(f"AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ìš”ì²­: {request.question}")
    
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸°ì—ì„œ ìµœì¢… ê²°ê³¼ë§Œ ì¶”ì¶œ
        final_result = None
        async for progress in concierge_service.generate_concierge_response_stream(request):
            if progress.stage == "completed" and progress.result:
                final_result = progress.result
                break
        
        if not final_result:
            raise HTTPException(status_code=500, detail="ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        return final_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.post("/concierge-stream")
async def get_concierge_stream(
    request: ConciergeRequest,
    concierge_service: NewsConciergeService = Depends(get_concierge_service)
):
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIê°€ ìƒì„±í•œ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    ì§„í–‰ ìƒí™©ê³¼ GPT-4ì˜ ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    logger = setup_logger("api.news.concierge_stream")
    logger.info(f"AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {request.question}")
    
    async def generate():
        try:
            async for progress in concierge_service.generate_concierge_response_stream_with_ai_streaming(request):
                # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
                data = progress.model_dump_json()
                yield f"data: {data}\n\n"
                
                # ì™„ë£Œ ì‹œ ì—°ê²° ì¢…ë£Œ
                if progress.stage == "completed" or progress.stage == "error":
                    break
                    
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            error_progress = ConciergeProgress(
                stage="error",
                progress=0,
                message=f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                current_task="ì˜¤ë¥˜ ì²˜ë¦¬"
            )
            yield f"data: {error_progress.model_dump_json()}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*"
        }
    )
    