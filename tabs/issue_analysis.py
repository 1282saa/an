import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re
from datetime import datetime, timedelta
from utils import make_api_request
from config import API_KEY, ENDPOINTS

def render(selected_date, days_to_analyze, debug_mode):
    """'ì´ìŠˆ ë¶„ì„' íƒ­ UI ë° ë¡œì§ ë Œë”ë§"""
    st.header("ğŸ” ì´ìŠˆ ìƒì„¸ ë¶„ì„")

    # ì„¸ì…˜ ìƒíƒœì—ì„œ ë¶„ì„í•  ì´ìŠˆ ê°€ì ¸ì˜¤ê¸°
    if "selected_issue" not in st.session_state:
        st.info("ë¨¼ì € 'ì˜¤ëŠ˜ì˜ ì´ìŠˆ' íƒ­ì—ì„œ ë¶„ì„í•  ì´ìŠˆë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        # í•„ìš”ì‹œ ê¸°ë³¸ ì´ìŠˆ ë¡œë”© ë¡œì§ ì¶”ê°€ ê°€ëŠ¥ (ì„ íƒ ì‚¬í•­)
        # ì˜ˆ: ê¸°ë³¸ ë¡œë”© ë¡œì§
        # with st.spinner("ê¸°ë³¸ ì´ìŠˆ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        #     # ... (today_issues.pyì˜ ì´ìŠˆ ë¡œë”©ê³¼ ìœ ì‚¬í•œ ë¡œì§)
        #     if topics:
        #         st.session_state.selected_issue = topics[0] # ì²« ë²ˆì§¸ ì´ìŠˆë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ
        #     else:
        #         st.warning("ì„ íƒí•  ìˆ˜ ìˆëŠ” ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
        #         st.stop()
        st.stop() # ì„ íƒëœ ì´ìŠˆ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨

    selected_issue = st.session_state.selected_issue
    topic = selected_issue.get('topic', 'ì œëª© ì—†ìŒ')
    st.subheader(f"ì„ íƒí•œ ì´ìŠˆ: {topic}")

    # ì´ìŠˆ í‚¤ì›Œë“œ í‘œì‹œ
    keywords = selected_issue.get("topic_keyword", "").split(",")[:15]
    st.write("**ì´ìŠˆ í‚¤ì›Œë“œ:**")
    st.write(", ".join(keywords))

    # ë¶„ì„ ì‹œì‘ ë²„íŠ¼
    if st.button("ì´ìŠˆ ë¶„ì„ ì‹œì‘", key="start_analysis"):
        with st.spinner("ì´ìŠˆë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘... (ìµœëŒ€ 2ë¶„ ì†Œìš”)"):
            # ë¶„ì„ ê¸°ê°„ ì„¤ì •
            end_date_obj = selected_date
            start_date_obj = end_date_obj - timedelta(days=7) # ìµœê·¼ 1ì£¼ì¼
            start_date_str = start_date_obj.strftime("%Y-%m-%d")
            end_date_str = end_date_obj.strftime("%Y-%m-%d")

            # í‚¤ì›Œë“œ ì„¤ì • (ìƒìœ„ 3ê°œ)
            main_keywords = keywords[:3]
            query = " AND ".join(main_keywords)

            # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
            analysis_result = {
                "issue_topic": topic,
                "query": query,
                "period": f"{start_date_str} ~ {end_date_str}",
                "related_news": [],
                "related_keywords": [],
                "timeline": None # ì´ˆê¸°ê°’ None
            }

            # 1. ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
            search_data = {
                "argument": {
                    "query": query,
                    "published_at": {"from": start_date_str, "until": end_date_str},
                    "provider": ["02100201"], # ì„œìš¸ê²½ì œ
                    "sort": {"date": "desc"},
                    "return_from": 0,
                    "return_size": 50, # ë‰´ìŠ¤ ê°œìˆ˜ ì¦ê°€
                    "fields": ["title", "content", "published_at", "provider", "category", "byline", "hilight", "news_id"]
                }
            }
            news_result = make_api_request(API_KEY, ENDPOINTS.get("search_news"), search_data, debug=debug_mode)
            if news_result:
                analysis_result["related_news"] = news_result.get("documents", [])

            # 2. ì—°ê´€ì–´ ë¶„ì„
            wordcloud_data = {
                "argument": {
                    "query": query,
                    "published_at": {"from": start_date_str, "until": end_date_str},
                    "provider": ["02100201"]
                }
            }
            wordcloud_result = make_api_request(API_KEY, ENDPOINTS.get("word_cloud"), wordcloud_data, debug=debug_mode)
            if wordcloud_result:
                analysis_result["related_keywords"] = wordcloud_result.get("nodes", [])

            # 3. ì‹œê°„ë³„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ (ì „ì²´ ë¶„ì„ ê¸°ê°„)
            timeline_start = selected_date - timedelta(days=days_to_analyze)
            timeline_end = selected_date
            timeline_data = {
                "argument": {
                    "query": query,
                    "published_at": {
                        "from": timeline_start.strftime("%Y-%m-%d"),
                        "until": timeline_end.strftime("%Y-%m-%d")
                    },
                    "provider": ["02100201"],
                    "interval": "day",
                    "normalize": False # ì‹¤ì œ ë¹ˆë„ìˆ˜ í™•ì¸
                }
            }
            timeline_result = make_api_request(API_KEY, ENDPOINTS.get("time_line"), timeline_data, debug=debug_mode)
            if timeline_result:
                analysis_result["timeline"] = timeline_result

            # ë¶„ì„ ê²°ê³¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.analysis_result = analysis_result
            st.success("ì´ìŠˆ ë¶„ì„ ì™„ë£Œ!")

    # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    if "analysis_result" in st.session_state:
        display_analysis_results(st.session_state.analysis_result)

def display_analysis_results(analysis_result):
    """ë¶„ì„ ê²°ê³¼ë¥¼ íƒ­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í‘œì‹œ"""
    # ë¶„ì„ ì •ë³´ ìš”ì•½ í‘œì‹œ
    st.markdown("--- ")
    st.markdown(f"**ë¶„ì„ ëŒ€ìƒ:** {analysis_result.get('issue_topic', 'N/A')}")
    st.markdown(f"**ë¶„ì„ ê¸°ê°„:** {analysis_result.get('period', 'N/A')}")
    st.markdown(f"**ì‚¬ìš© í‚¤ì›Œë“œ:** `{analysis_result.get('query', 'N/A')}`")
    st.markdown("--- ")

    # ë¶„ì„ ê²°ê³¼ íƒ­ ìƒì„±
    analysis_tab1, analysis_tab2, analysis_tab3 = st.tabs([
        "ğŸ“° ê´€ë ¨ ê¸°ì‚¬", "ğŸ”‘ í‚¤ì›Œë“œ ë¶„ì„", "ğŸ“ˆ ì‹œê°„ë³„ ì¶”ì´"
    ])

    # íƒ­ 1: ê´€ë ¨ ê¸°ì‚¬
    with analysis_tab1:
        render_related_news(analysis_result.get("related_news", []))

    # íƒ­ 2: í‚¤ì›Œë“œ ë¶„ì„
    with analysis_tab2:
        render_keyword_analysis(analysis_result.get("related_keywords", []))

    # íƒ­ 3: ì‹œê°„ë³„ ì¶”ì´
    with analysis_tab3:
        render_timeline_analysis(analysis_result.get("timeline"))

def render_related_news(related_news):
    """ê´€ë ¨ ê¸°ì‚¬ ëª©ë¡ ë Œë”ë§"""
    st.subheader(f"ê´€ë ¨ ê¸°ì‚¬ ëª©ë¡ ({len(related_news)}ê°œ)")

    if not related_news:
        st.info("ë¶„ì„ ê¸°ê°„ ë‚´ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
    items_per_page = 10
    total_pages = (len(related_news) + items_per_page - 1) // items_per_page
    page_key = "related_news_page"
    if page_key not in st.session_state: st.session_state[page_key] = 1

    # í˜ì´ì§€ ì„ íƒ
    cols = st.columns([1, 2, 1])
    if cols[0].button("â—€ ì´ì „", key="prev_news_page"):
        if st.session_state[page_key] > 1:
            st.session_state[page_key] -= 1
    cols[1].write(f"í˜ì´ì§€: {st.session_state[page_key]} / {total_pages}")
    if cols[2].button("ë‹¤ìŒ â–¶", key="next_news_page"):
        if st.session_state[page_key] < total_pages:
            st.session_state[page_key] += 1

    # í˜„ì¬ í˜ì´ì§€ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ ìŠ¬ë¼ì´ì‹±
    start_idx = (st.session_state[page_key] - 1) * items_per_page
    end_idx = start_idx + items_per_page
    current_page_news = related_news[start_idx:end_idx]

    # ê´€ë ¨ ê¸°ì‚¬ í‘œì‹œ
    for i, news in enumerate(current_page_news):
        idx_overall = start_idx + i + 1
        with st.expander(f"{idx_overall}. {news.get('title', 'ì œëª© ì—†ìŒ')} ({news.get('provider', 'ë¯¸ìƒ')})", expanded=(i==0)):
            st.caption(f"ë°œí–‰: {news.get('published_at', '')[:10]} | ê¸°ì: {news.get('byline', 'N/A')} | ì¹´í…Œê³ ë¦¬: {news.get('category', 'N/A')}")

            # í•˜ì´ë¼ì´íŠ¸
            if "hilight" in news and news["hilight"]:
                st.markdown("**í•˜ì´ë¼ì´íŠ¸:**")
                st.markdown(f"> {news['hilight']}")

            # ë‚´ìš© ìš”ì•½ (ì²« 3ë¬¸ì¥ ë˜ëŠ” 200ì)
            if "content" in news and news["content"]:
                st.markdown("**ë‚´ìš© ìš”ì•½:**")
                content = news.get("content", "")
                sentences = re.split(r'(?<=[.!?])\s+', content)
                summary = " ".join(sentences[:3])
                if len(summary) > 200:
                    summary = summary[:197] + "..."
                st.write(summary)

            # ì „ì²´ ë‚´ìš© ë³´ê¸° í† ê¸€
            if "content" in news and news["content"]:
                 if st.checkbox(f"ì „ì²´ ë‚´ìš© ë³´ê¸° #{idx_overall}", key=f"show_content_{news.get('news_id', idx_overall)}"):
                     st.markdown("**ì „ì²´ ë‚´ìš©:**")
                     st.text_area("", news["content"], height=200, disabled=True)

    st.divider()

def render_keyword_analysis(related_keywords):
    """í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ë Œë”ë§"""
    st.subheader("ì—°ê´€ í‚¤ì›Œë“œ ë¶„ì„")

    if not related_keywords:
        st.info("ì—°ê´€ì–´ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    related_keywords_df = pd.DataFrame([
        {"í‚¤ì›Œë“œ": item.get("name", ""), "ì¤‘ìš”ë„": round(item.get("weight", 0), 2)}
        for item in related_keywords
    ]).sort_values(by="ì¤‘ìš”ë„", ascending=False).reset_index(drop=True)

    st.dataframe(related_keywords_df, use_container_width=True)

    # ìƒìœ„ í‚¤ì›Œë“œ ë§‰ëŒ€ ì°¨íŠ¸ (ìƒìœ„ 15ê°œ)
    st.markdown("**ìƒìœ„ ì—°ê´€ í‚¤ì›Œë“œ (Top 15)**")
    top_keywords = related_keywords_df.head(15)

    try:
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.barplot(data=top_keywords, x="ì¤‘ìš”ë„", y="í‚¤ì›Œë“œ", palette="viridis", ax=ax)
        ax.set_title("ìƒìœ„ ì—°ê´€ í‚¤ì›Œë“œ ì¤‘ìš”ë„")
        ax.tick_params(axis='y', labelsize=10)
        plt.tight_layout()
        st.pyplot(fig)
    except Exception as e:
        st.error(f"í‚¤ì›Œë“œ ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.dataframe(top_keywords) # ì˜¤ë¥˜ ì‹œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ëŒ€ì²´ í‘œì‹œ

def render_timeline_analysis(timeline_data):
    """ì‹œê°„ë³„ ì¶”ì´ ë¶„ì„ ê²°ê³¼ ë Œë”ë§"""
    st.subheader("ê¸°ê°„ë³„ í‚¤ì›Œë“œ ì–¸ê¸‰ ì¶”ì´")

    if not timeline_data or "time_line" not in timeline_data or not timeline_data["time_line"]:
        st.info("ì‹œê°„ë³„ ì¶”ì´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë‚ ì§œ í¬ë§· ë³€í™˜ í•¨ìˆ˜
    def format_date(date_str):
        try:
            if len(date_str) == 8: return datetime.strptime(date_str, "%Y%m%d")
            if len(date_str) == 6: return datetime.strptime(date_str, "%Y%m")
            return pd.to_datetime(date_str) # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
        except ValueError:
            return None # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

    # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ë‚ ì§œ ë³€í™˜
    timeline_df_data = []
    for item in timeline_data["time_line"]:
        parsed_date = format_date(item.get("label"))
        if parsed_date:
            timeline_df_data.append({"ë‚ ì§œ": parsed_date, "ì–¸ê¸‰ íšŸìˆ˜": item.get("hits", 0)})

    if not timeline_df_data:
        st.error("íƒ€ì„ë¼ì¸ ë‚ ì§œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    timeline_df = pd.DataFrame(timeline_df_data).sort_values(by="ë‚ ì§œ")

    st.dataframe(timeline_df.style.format({"ë‚ ì§œ": '{:%Y-%m-%d}'}), use_container_width=True)

    # íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ìƒì„±
    st.markdown("**í‚¤ì›Œë“œ ì–¸ê¸‰ ì¶”ì´ ì°¨íŠ¸**")
    try:
        fig, ax = plt.subplots(figsize=(12, 6))
        sns.lineplot(data=timeline_df, x="ë‚ ì§œ", y="ì–¸ê¸‰ íšŸìˆ˜", marker="o", ax=ax)

        # ì°¨íŠ¸ ìŠ¤íƒ€ì¼ë§
        plt.xticks(rotation=45)
        ax.set_title("ì¼ìë³„ í‚¤ì›Œë“œ ì–¸ê¸‰ íšŸìˆ˜")
        ax.set_xlabel("ë‚ ì§œ")
        ax.set_ylabel("ì–¸ê¸‰ íšŸìˆ˜")
        plt.grid(True, axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        st.pyplot(fig)
    except Exception as e:
        st.error(f"íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.dataframe(timeline_df) # ì˜¤ë¥˜ ì‹œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ëŒ€ì²´ í‘œì‹œ 